{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Llp9ATaspf44"
      },
      "source": [
        "LeNet5 model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UW2DiV4Npf46"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import tqdm as notebook_tqdm\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms \n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import datasets \n",
        "from datetime import datetime\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9lb72YJCpf48"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zV3lntQ8pf49"
      },
      "outputs": [],
      "source": [
        "# defining hyperparameters \n",
        "epochs = 10 \n",
        "#epochs = 40   #the nn will train 40 times over all 50.000 images and validate itself with 12.000 images x 40 times) \n",
        "learning_rate = 0.0001 #how much the weight will be updated each time \n",
        "batch_size = 32 \n",
        "classes = 43 \n",
        "img_size = 32\n",
        "random_seed = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(<torch.utils.data.dataloader.DataLoader object at 0x7f3f8b444fa0>, <torch.utils.data.dataloader.DataLoader object at 0x7f3eefae3dc0>)\n"
          ]
        }
      ],
      "source": [
        "def get_train_valid_loader(\n",
        "                           batch_size,\n",
        "                           augment,\n",
        "                           random_seed,\n",
        "                           valid_size=0.1,\n",
        "                           shuffle=True,\n",
        "                           num_workers=2):\n",
        "\n",
        "    error_msg = \"[!] valid_size should be in the range [0, 1].\"\n",
        "    assert ((valid_size >= 0) and (valid_size <= 1)), error_msg\n",
        "\n",
        "    normalize = transforms.Normalize((0.3403, 0.3121, 0.3214),\n",
        "                                    (0.2724, 0.2608, 0.2669))\n",
        "\n",
        "    # define transforms\n",
        "    valid_transform = transforms.Compose([transforms.Resize((32,32)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      ])\n",
        "   \n",
        "    train_transform = transforms.Compose([transforms.Resize((32,32)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       normalize])\n",
        "\n",
        "    # load the dataset\n",
        "\n",
        "    base_dataset = datasets.ImageFolder(\n",
        "        root='/volumes1/thesis/notebooks/data/gtsrb/GTSRB/Training', transform=train_transform,\n",
        "    )\n",
        "\n",
        "    # TODO\n",
        "    split_datasets = torch.utils.data.random_split(base_dataset, [0.20,0.8])\n",
        "    global val_dataset \n",
        "    val_dataset = split_datasets[0]\n",
        "    train_dataset = split_datasets[1]\n",
        "    \n",
        "\n",
        "    global num_train \n",
        "    num_train= len(train_dataset)\n",
        "    indices = list(range(num_train))\n",
        "    global split \n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "\n",
        "    #train_idx, valid_idx = indices[split:], indices[:split]\n",
        "    #train_sampler = SubsetRandomSampler(train_idx)\n",
        "    #valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "\n",
        "    global train_loader \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size,\n",
        "        num_workers=num_workers, \n",
        "        #sampler = train_sampler\n",
        "    )\n",
        "    global valid_loader \n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset, batch_size=batch_size,\n",
        "        num_workers=num_workers, \n",
        "        #sampler = valid_sampler\n",
        "    )\n",
        "\n",
        "    return train_loader, valid_loader\n",
        "\n",
        "print(get_train_valid_loader(batch_size = 64, augment = True, random_seed = 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7W6M-TLVpf49"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, data_loader, device):\n",
        "    \n",
        "    correct_pred = 0 \n",
        "    n = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for X, y_true in data_loader:\n",
        "\n",
        "            X = X.to(device)\n",
        "            y_true = y_true.to(device)\n",
        "\n",
        "            _, y_prob = model(X)\n",
        "            _, predicted_labels = torch.max(y_prob, 1)\n",
        "\n",
        "            n += y_true.size(0)\n",
        "            correct_pred += (predicted_labels == y_true).sum()\n",
        "\n",
        "    return correct_pred.float() / n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JYBO4RN-pf49"
      },
      "outputs": [],
      "source": [
        "def plot_losses(train_losses, valid_losses):\n",
        "\n",
        "    train_losses = np.array(train_losses) \n",
        "    valid_losses = np.array(valid_losses)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize = (8, 4.5))\n",
        "\n",
        "    ax.plot(train_losses, color='blue', label='Training loss') \n",
        "    ax.plot(valid_losses, color='red', label='Validation loss')\n",
        "    ax.set(title=\"Loss over epochs\", \n",
        "            xlabel='Epoch',\n",
        "            ylabel='Loss') \n",
        "    ax.legend()\n",
        "    fig.show()\n",
        "    \n",
        "    # change the plot style to default\n",
        "    plt.style.use('default')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HWdsFAFepf49"
      },
      "outputs": [],
      "source": [
        "# train function\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, device):\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    \n",
        "    for X, y_true in train_loader:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        X = X.to(device)\n",
        "        y_true = y_true.to(device)\n",
        "    \n",
        "        # Forward pass\n",
        "        y_hat, _ = model(X) \n",
        "        loss = criterion(y_hat, y_true) \n",
        "        running_loss += loss.item() * X.size(0)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    return model, optimizer, epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dlfYbHs5pf4-"
      },
      "outputs": [],
      "source": [
        "# validation function, without a learning step (backward pass)\n",
        "\n",
        "def validate(valid_loader, model, criterion, device):\n",
        "   \n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    \n",
        "    for X, y_true in valid_loader:\n",
        "    \n",
        "        X = X.to(device)\n",
        "        y_true = y_true.to(device)\n",
        "\n",
        "        # Forward pass and record loss\n",
        "        y_hat, _ = model(X) \n",
        "        loss = criterion(y_hat, y_true) \n",
        "        running_loss += loss.item() * X.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
        "        \n",
        "    return model, epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cZArmj8gpf4-"
      },
      "outputs": [],
      "source": [
        "# training function\n",
        "\n",
        "def training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, device, print_every=1):\n",
        "    # set objects for storing metrics\n",
        "    best_loss = 1e10\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        " \n",
        "    # Train model\n",
        "    for epoch in range(0, epochs):\n",
        "\n",
        "        # training\n",
        "        model, optimizer, train_loss = train(train_loader, model, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # validation\n",
        "        with torch.no_grad():\n",
        "            model, valid_loss = validate(valid_loader, model, criterion, device)\n",
        "            valid_losses.append(valid_loss)\n",
        "\n",
        "        if epoch % print_every == (print_every - 1):\n",
        "            \n",
        "            train_acc = get_accuracy(model, train_loader, device=device)\n",
        "            valid_acc = get_accuracy(model, valid_loader, device=device)\n",
        "                \n",
        "            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n",
        "                  f'Epoch: {epoch}\\t'\n",
        "                  f'Train loss: {train_loss:.4f}\\t'\n",
        "                  f'Valid loss: {valid_loss:.4f}\\t'\n",
        "                  f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
        "                  f'Valid accuracy: {100 * valid_acc:.2f}')\n",
        "\n",
        "    plot_losses(train_losses, valid_losses)\n",
        "    \n",
        "    return model, optimizer, (train_losses, valid_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R8ioVg8ypf4_"
      },
      "outputs": [],
      "source": [
        "# model function \n",
        "class LeNet5(nn.Module):\n",
        "\n",
        "    def __init__(self, n_classes):\n",
        "        super(LeNet5, self).__init__()\n",
        "        \n",
        "        self.feature_extractor = nn.Sequential(            \n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1),\n",
        "            nn.Tanh(),\n",
        "            nn.AvgPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
        "            nn.Tanh(),\n",
        "            nn.AvgPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=120, out_features=84),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=84, out_features=n_classes),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        logits = self.classifier(x)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return logits, probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "saNqjNZnpf4_"
      },
      "outputs": [],
      "source": [
        "\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "model = LeNet5(classes).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "daTF2ggkpf4_",
        "outputId": "40ca8fc1-a104-4030-b841-bcc1d28665f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "09:54:45 --- Epoch: 0\tTrain loss: 3.3951\tValid loss: 2.9724\tTrain accuracy: 23.85\tValid accuracy: 23.31\n",
            "09:54:52 --- Epoch: 1\tTrain loss: 2.5789\tValid loss: 2.1893\tTrain accuracy: 39.82\tValid accuracy: 40.22\n",
            "09:54:59 --- Epoch: 2\tTrain loss: 1.9568\tValid loss: 1.6994\tTrain accuracy: 52.71\tValid accuracy: 54.02\n",
            "09:55:06 --- Epoch: 3\tTrain loss: 1.5372\tValid loss: 1.3328\tTrain accuracy: 64.20\tValid accuracy: 65.05\n",
            "09:55:14 --- Epoch: 4\tTrain loss: 1.2200\tValid loss: 1.0703\tTrain accuracy: 72.95\tValid accuracy: 73.35\n",
            "09:55:21 --- Epoch: 5\tTrain loss: 0.9917\tValid loss: 0.8813\tTrain accuracy: 79.08\tValid accuracy: 78.77\n",
            "09:55:28 --- Epoch: 6\tTrain loss: 0.8224\tValid loss: 0.7392\tTrain accuracy: 83.24\tValid accuracy: 82.62\n",
            "09:55:35 --- Epoch: 7\tTrain loss: 0.6923\tValid loss: 0.6294\tTrain accuracy: 86.34\tValid accuracy: 85.40\n",
            "09:55:43 --- Epoch: 8\tTrain loss: 0.5909\tValid loss: 0.5434\tTrain accuracy: 88.48\tValid accuracy: 87.82\n",
            "09:55:50 --- Epoch: 9\tTrain loss: 0.5108\tValid loss: 0.4751\tTrain accuracy: 90.04\tValid accuracy: 89.34\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x450 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model, optimizer, _ = training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41b-5DVipf5A"
      },
      "source": [
        "# Find adversarial samples with ART."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIlmX_-3uYHG",
        "outputId": "08b58145-0165-4057-e935-ebcad8130ba9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LeNet5(\n",
              "  (feature_extractor): Sequential(\n",
              "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (1): Tanh()\n",
              "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (4): Tanh()\n",
              "    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "    (6): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (7): Tanh()\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
              "    (1): Tanh()\n",
              "    (2): Linear(in_features=84, out_features=43, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We have a trained model from the code above\n",
        "# Put everything on CPU\n",
        "model.to('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll4roONYvUBa"
      },
      "source": [
        "### Find a single adversarial example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217,
          "referenced_widgets": [
            "a29e4250cf2b4740b22d45c4b49dbde4",
            "920ac0b645d14201b6f5b4c5eaada5bf",
            "667aff09a27340d99aa6a349e126f467",
            "e6b44dd988d44a2986148b05697cd117",
            "611704eecac2468090acdf34f968398a",
            "1da7641faf794db6aceb148d770ee5b3",
            "6800badfc70d4a88a9fafe792e90e865",
            "0b5f5d84ca7b4892a116d3c1809c3900",
            "0ed1635161f9465d929321bdf5a61bb5",
            "466bfd965bdb4faabce67e0a514c4be7",
            "e23488f6634946eea008ab05c227a664",
            "8e382e0c517440f8b2523a3be3bbbb5c",
            "a2d743c8449b40cf8cfb3e035e61e955",
            "f017d0bda5884853af951976473fbdfd",
            "fbd35017269f42f09dfd3d599e3c6f5e",
            "47a4f2100ea74ff18b3995e0a2ac9b88",
            "44120a9964c440f9a10e84b8713d7f30",
            "7d2c234dfd024d29adbdfa8bd0a8e2c8",
            "0d0304a706b34fc782b1615702a1313f",
            "2d6b9502aa2b44248a3b7b2cb960b416",
            "d2156e5c21db40fa87db3e402c0706c0",
            "1103c60a8f1547eaa1b984680e99aca0",
            "4d4e1deafdd94b62a0ea6684d314afed",
            "e3d2b36ffc3047a285c032b080a7238c",
            "e57480d4db9c40ab8265545d321f4e78",
            "6e13f1b052904eb5a238ac7f699ed613",
            "007c5c3eac20492a8ea516282a38af93",
            "8070320f4200456b87eb791abdbcaf63",
            "2843059f898c40c5905d6cc773c44186",
            "d8148ba24cd6449b917480f14a76180f",
            "31b5ff05bf3444aebb5e487bf14a206a",
            "0e2a0c33c3af46ccab2a8c1bb52cdb47",
            "556cf98981ec4fff8e43109a5df15108"
          ]
        },
        "id": "DlC_dX-Jpf4_",
        "outputId": "3c7ecfca-e0c5-46dd-b7c2-ba0601e85790"
      },
      "outputs": [],
      "source": [
        "# loading dataset \n",
        "transforms = transforms.Compose([transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor()])\n",
        "    \n",
        "test_dataset = torchvision.datasets.GTSRB(\n",
        "    root='./data', split = 'test', transform=transforms, download=True)\n",
        "\n",
        "# train loader \n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=batch_size\n",
        ", shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([3, 32, 32]), tensor(0.0392), tensor(0.7176))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x, y_true = test_dataset[0]\n",
        "x.shape, x.min(), x.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What does X look like?\n",
        "\n",
        "If we want to correctly display X, we need to first reverse the normalization transform defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "ZZaA9rZ7waeG"
      },
      "outputs": [],
      "source": [
        "class NormalizeInverse(torchvision.transforms.Normalize):\n",
        "    \"\"\"\n",
        "    Undoes the normalization and returns the reconstructed images in the input domain.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mean, std):\n",
        "        mean = torch.as_tensor(mean)\n",
        "        std = torch.as_tensor(std)\n",
        "        std_inv = 1 / (std + 1e-7)\n",
        "        mean_inv = -mean * std_inv\n",
        "        super().__init__(mean=mean_inv, std=std_inv)\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return super().__call__(tensor.clone())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "lLaMWZFrwcU_"
      },
      "outputs": [],
      "source": [
        "inverse_normalize = NormalizeInverse(mean=(0.3403, 0.3121, 0.3214), std=(0.2724, 0.2608, 0.2669))  # Same mean and std values as defined in the original transforms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "dG4MXs2vv_Hx",
        "outputId": "15b0c606-4e59-4f1b-e913-a5ebec85546b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32>"
            ]
          },
          "execution_count": 182,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchvision\n",
        "torchvision.transforms.ToPILImage()(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMPMsciyxpz1"
      },
      "source": [
        "We will be looking for adversarial examples on the original inputs, not the normalized inputs. The original inputs (the ones that have not been normalized yet) have values between [0, 1]. The adversarial samples should also have values in the same range.\n",
        "\n",
        "However, our model expects inputs that are normalized.\n",
        "\n",
        "We therefore write a small wrapper around our model that accepts unnormalized inputs, but does the normalization before passing it to the original model. This allows ART to search for adversarial samples in the original [0,1] range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "27kdGd2KySvM"
      },
      "outputs": [],
      "source": [
        "class WrappedModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.trained_model = model  # \"model\" is our trained model\n",
        "    self.trained_model.to(\"cpu\")\n",
        "    self.normalize = torchvision.transforms.Normalize((0.3403, 0.3121, 0.3214), (0.2724, 0.2608, 0.2669))\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    # x comes in as a Tensor of shape [3, 32, 32]\n",
        "    # Manually add a batch dimension\n",
        "    x = torch.unsqueeze(x, dim=0)\n",
        "    # Shape of x is now [1, 3, 32, 32]\n",
        "    # Normalize the input\n",
        "    x_normalized = self.normalize(x)\n",
        "    # Return only the logits\n",
        "    logits, _ = self.trained_model(x_normalized)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "NC-xqrpZsk_U"
      },
      "outputs": [],
      "source": [
        "from art.attacks.evasion import FastGradientMethod\n",
        "from art.estimators.classification import PyTorchClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "kvaRJqZqt1Zl"
      },
      "outputs": [],
      "source": [
        "# We again wrap our (already wrapped) model in a PyTorchClassifier, which ART knows how to use\n",
        "classifier = PyTorchClassifier(\n",
        "      model=WrappedModel(),\n",
        "      clip_values=(0.0, 1.0),  # The adversarial sample tensor values will be within these values\n",
        "      loss=criterion,  # defined above\n",
        "      optimizer=optimizer,  # defined above\n",
        "      input_shape=(3, 32, 32),\n",
        "      nb_classes=43,\n",
        "      device_type=\"cpu\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FGSM attack - 1st attempt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "LoTNEBuUzzCz"
      },
      "outputs": [],
      "source": [
        "# Define an ART attack and create a generator \n",
        "attack = FastGradientMethod(estimator=classifier, eps=0.1)\n",
        "#x_test_adv = attack.generate(x_test.numpy())\n",
        "\n",
        "#predictions = classifier.predict(x_test_adv)\n",
        "#predictions = torch.from_numpy(predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = x.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hLMb_sY0Ugo",
        "outputId": "b3d57b7f-c1bf-4162-bbfe-7df6b4886054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(numpy.ndarray, 0.3284639, 0.51294005)"
            ]
          },
          "execution_count": 188,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ART expects inputs to be numpy arrays!\n",
        "# Find adversarial samples in the original input domain - which is unnormalized\n",
        "benign_x = inverse_normalize(x)\n",
        "benign_x = benign_x.cpu().numpy()\n",
        "type(benign_x), benign_x.min(), benign_x.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 32, 32)"
            ]
          },
          "execution_count": 189,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "benign_x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "XeaWN0yO0pnL"
      },
      "outputs": [],
      "source": [
        "x_adversarial = attack.generate(benign_x)    # lijst van maken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNCLxTYX2aKZ",
        "outputId": "6a923ec3-2e0a-40c2-9228-96c26be6fb4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 32, 32)"
            ]
          },
          "execution_count": 191,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_adversarial.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y36bYZ0G00Zh",
        "outputId": "194784c4-a3a4-478e-e906-db171c6747b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.23050943, 0.6129401)"
            ]
          },
          "execution_count": 192,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_adversarial.min(), x_adversarial.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPT3xBxq46CS"
      },
      "source": [
        "### Check the predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "yHUrUKVn7oJm",
        "outputId": "3ee704ed-f74c-4dfe-c943-57bd15280bb8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32>"
            ]
          },
          "execution_count": 193,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torchvision.transforms.ToPILImage()(torch.Tensor(x_adversarial))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "iShXNO3S5B28"
      },
      "outputs": [],
      "source": [
        "test_model = WrappedModel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFW80h6r5HUW",
        "outputId": "80762482-0d70-4d5a-87bc-e5c05311252a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(13)"
            ]
          },
          "execution_count": 195,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.argmax(test_model(torch.Tensor(x_adversarial)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 196,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "import os "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#def get_accuracy_adv(data_loader):\n",
        "    \n",
        "    correct_pred = 0 \n",
        "    n = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for X, y_true in data_loader:\n",
        "\n",
        "            X = X.to('cpu')\n",
        "            y_true = y_true.to('cpu')\n",
        "\n",
        "            classifier = PyTorchClassifier(\n",
        "            model=WrappedModel(),\n",
        "            clip_values=(0.0, 1.0),  # The adversarial sample tensor values will be within these values\n",
        "            loss=criterion,  # defined above\n",
        "            optimizer=optimizer,  # defined above\n",
        "            input_shape=(3, 32, 32),\n",
        "            nb_classes=43,\n",
        "            device_type=\"cpu\"\n",
        "            )\n",
        "            attack = FastGradientMethod(estimator=classifier, eps=0.1)\n",
        "            x_test_adv = X.cpu().numpy()\n",
        "            x_adversarial = attack.generate(x_test_adv)\n",
        "            test_model = WrappedModel()\n",
        "\n",
        "            _, y_prob = test_model(X)\n",
        "            _, predicted_labels = torch.argmax(test_model(torch.Tensor(x_adversarial, 1)))\n",
        "            \n",
        "            n += y_true.size(0)\n",
        "            correct_pred += (predicted_labels == y_true).sum()\n",
        "\n",
        "    return correct_pred.float() / n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#get_accuracy_adv(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#def generate_attack(dataloader):\n",
        "  open('/volumes1/thesis/notebooks/data/gtsrb/GTSRB/adversarial_samples')\n",
        "  \n",
        "  true_labels = []\n",
        "\n",
        "\n",
        "  for X, y_true in dataloader:\n",
        "    X = x.to('cpu')\n",
        "    y_true = y_true.to('çpu')\n",
        "\n",
        "  \n",
        "    classifier = PyTorchClassifier(\n",
        "      model=WrappedModel(),\n",
        "      clip_values=(0.0, 1.0),  # The adversarial sample tensor values will be within these values\n",
        "      loss=criterion,  # defined above\n",
        "      optimizer=optimizer,  # defined above\n",
        "      input_shape=(3, 32, 32),\n",
        "      nb_classes=43,\n",
        "      device_type=\"cpu\"\n",
        "    )\n",
        "    # Define an ART attack and create a generator \n",
        "    attack = FastGradientMethod(estimator=classifier, eps=0.1)\n",
        "\n",
        "    # Find an adversarial sample\n",
        "    # ART expects inputs to be numpy arrays!\n",
        "\n",
        "    # Find adversarial samples in the original input domain - which is unnormalize\n",
        "    benign_x = x.cpu().numpy()  # convert torch to numpy\n",
        "    #type(benign_x), benign_x.min(), benign_x.max()\n",
        "    x_adversarial = attack.generate(benign_x) \n",
        "    torchvision.transforms.ToPILImage()(torch.Tensor(x_adversarial))\n",
        "    #attacks.append(adversarial_sample)\n",
        "    test_model = WrappedModel()\n",
        "    prediction = torch.argmax(test_model(torch.Tensor(x_adversarial)))\n",
        "    \n",
        "    accuracy = get_accuracy_adv()\n",
        "\n",
        "\n",
        "    close('/volumes1/thesis/notebooks/data/gtsrb/GTSRB/adversarial_samples')\n",
        "    return prediction, y, correct_pred, accuracy, true_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#generate_attack(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#def display_adv_samples(x):\n",
        "        \n",
        "    #return torchvision.transforms.ToPILImage()(torch.Tensor(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#display_adv_samples(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import cv2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##for x,y in test_loader:\n",
        "    #print(cv2.imread(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Carlini & Wagner attack - 1st attempt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "from art.attacks.evasion import CarliniL0Method, CarliniL2Method, CarliniLInfMethod, CarliniWagnerASR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[0.45490196, 0.45490196, 0.4627451 , ..., 0.39607844,\n",
              "         0.39215687, 0.35686275],\n",
              "        [0.45490196, 0.45490196, 0.45882353, ..., 0.47058824,\n",
              "         0.4745098 , 0.47058824],\n",
              "        [0.45882353, 0.45882353, 0.4509804 , ..., 0.4745098 ,\n",
              "         0.47843137, 0.47058824],\n",
              "        ...,\n",
              "        [0.45882353, 0.45490196, 0.45490196, ..., 0.45490196,\n",
              "         0.45490196, 0.4627451 ],\n",
              "        [0.45490196, 0.44705883, 0.44705883, ..., 0.44705883,\n",
              "         0.45490196, 0.4509804 ],\n",
              "        [0.4392157 , 0.4392157 , 0.4627451 , ..., 0.4509804 ,\n",
              "         0.45882353, 0.44705883]],\n",
              "\n",
              "       [[0.54901963, 0.5411765 , 0.5411765 , ..., 0.4627451 ,\n",
              "         0.4745098 , 0.43529412],\n",
              "        [0.5529412 , 0.54901963, 0.5529412 , ..., 0.5568628 ,\n",
              "         0.5568628 , 0.54901963],\n",
              "        [0.5568628 , 0.5529412 , 0.54901963, ..., 0.5647059 ,\n",
              "         0.5647059 , 0.56078434],\n",
              "        ...,\n",
              "        [0.5372549 , 0.5294118 , 0.5294118 , ..., 0.5372549 ,\n",
              "         0.53333336, 0.54509807],\n",
              "        [0.53333336, 0.5254902 , 0.52156866, ..., 0.53333336,\n",
              "         0.53333336, 0.54509807],\n",
              "        [0.5294118 , 0.5254902 , 0.5294118 , ..., 0.5411765 ,\n",
              "         0.5411765 , 0.54901963]],\n",
              "\n",
              "       [[0.6862745 , 0.6745098 , 0.6745098 , ..., 0.5803922 ,\n",
              "         0.5803922 , 0.5372549 ],\n",
              "        [0.6901961 , 0.68235296, 0.68235296, ..., 0.6901961 ,\n",
              "         0.68235296, 0.67058825],\n",
              "        [0.68235296, 0.68235296, 0.6784314 , ..., 0.7058824 ,\n",
              "         0.7019608 , 0.69411767],\n",
              "        ...,\n",
              "        [0.65882355, 0.64705884, 0.6431373 , ..., 0.6627451 ,\n",
              "         0.65882355, 0.67058825],\n",
              "        [0.6509804 , 0.654902  , 0.6509804 , ..., 0.654902  ,\n",
              "         0.6509804 , 0.65882355],\n",
              "        [0.654902  , 0.64705884, 0.64705884, ..., 0.6509804 ,\n",
              "         0.654902  , 0.6666667 ]]], dtype=float32)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CarliniL2Method(art.attacks.evasion.CarliniL2Method):\n",
        "    def __init__: \n",
        "        classifier = classifier \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "cw0_attack = CarliniL0Method(classifier, learning_rate = 0.0001)\n",
        "cw2_attack = CarliniL2Method(classifier, learning_rate = 0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "output with shape [1, 1, 32, 32] doesn't match the broadcast shape [1, 3, 32, 32]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cw0_attack\u001b[39m.\u001b[39;49mgenerate(x\u001b[39m.\u001b[39;49mnumpy())\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/art/attacks/evasion/carlini.py:970\u001b[0m, in \u001b[0;36mCarliniL0Method.generate\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39m# No labels provided, use model prediction as correct class\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     y \u001b[39m=\u001b[39m get_labels_np_array(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimator\u001b[39m.\u001b[39;49mpredict(x, batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size))\n\u001b[1;32m    972\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator\u001b[39m.\u001b[39mnb_classes \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    973\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    974\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis attack has not yet been tested for binary classification with a single output classifier.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    975\u001b[0m     )\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/art/estimators/classification/classifier.py:74\u001b[0m, in \u001b[0;36mInputFilter.__init__.<locals>.make_replacement.<locals>.replacement_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     73\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(lst)\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m fdict[func_name](\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/art/estimators/classification/pytorch.py:321\u001b[0m, in \u001b[0;36mPyTorchClassifier.predict\u001b[0;34m(self, x, batch_size, training_mode, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m begin, end \u001b[39m=\u001b[39m (\n\u001b[1;32m    316\u001b[0m     m \u001b[39m*\u001b[39m batch_size,\n\u001b[1;32m    317\u001b[0m     \u001b[39mmin\u001b[39m((m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m batch_size, x_preprocessed\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]),\n\u001b[1;32m    318\u001b[0m )\n\u001b[1;32m    320\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 321\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(torch\u001b[39m.\u001b[39;49mfrom_numpy(x_preprocessed[begin:end])\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device))\n\u001b[1;32m    322\u001b[0m output \u001b[39m=\u001b[39m model_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    323\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/art/estimators/classification/pytorch.py:1128\u001b[0m, in \u001b[0;36mPyTorchClassifier._make_model_wrapper.<locals>.ModelWrapper.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1125\u001b[0m         result\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m   1127\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m-> 1128\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(x)\n\u001b[1;32m   1129\u001b[0m     result\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m   1131\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# pragma: no cover\u001b[39;00m\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn [40], line 14\u001b[0m, in \u001b[0;36mWrappedModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(x, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Shape of x is now [1, 3, 32, 32]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# Normalize the input\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m x_normalized \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalize(x)\n\u001b[1;32m     15\u001b[0m \u001b[39m# Return only the logits\u001b[39;00m\n\u001b[1;32m     16\u001b[0m logits, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrained_model(x_normalized)\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/torchvision/transforms/transforms.py:270\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    263\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/torchvision/transforms/functional.py:360\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    358\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 360\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mnormalize(tensor, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, inplace\u001b[39m=\u001b[39;49minplace)\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:940\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[39mif\u001b[39;00m std\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    939\u001b[0m     std \u001b[39m=\u001b[39m std\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 940\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49msub_(mean)\u001b[39m.\u001b[39mdiv_(std)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: output with shape [1, 1, 32, 32] doesn't match the broadcast shape [1, 3, 32, 32]"
          ]
        }
      ],
      "source": [
        "cw0_attack.generate(x.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def attack_targeted(model, train_loader, x0, y0, target, alpha = 0.1, beta = 0.001, iterations = 1000, batch_size = 10):\n",
        "    \"\"\" Attack the original image and return adversarial example of target t\n",
        "        model: (pytorch model)\n",
        "        train_dataset: set of training data\n",
        "        (x0, y0): original image\n",
        "        t: target\n",
        "    \"\"\"\n",
        "    o_alpha = alpha\n",
        "    if (model.predict(x0) != y0):\n",
        "        print(\"Fail to classify the image. No need to attack.\")\n",
        "        return x0\n",
        "    # STEP I: find initial direction (theta, g_theta)\n",
        "    ''' \n",
        "    image, label = Variable(x0.cuda()), y0.cuda() \n",
        "    outputs = model(image)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    target = torch.randperm(10)\n",
        "    #print(predicted!=y0).nonzero()\n",
        "    '''\n",
        "    num_samples = 1000 \n",
        "    best_theta, g_theta = None, float('inf')\n",
        "    query_count = 0\n",
        "\n",
        "    #print(\"Searching for the initial direction on %d samples: \" % (num_samples))\n",
        "    timestart = time.time()\n",
        "    #samples = set(random.sample(range(len(train_dataset)), num_samples))\n",
        "    b_train_size = 1000\n",
        "    b_best_lbd = float('inf')\n",
        "    dim1 = x0.size()[0]\n",
        "    dim3 = x0.size()[2]\n",
        "    #for index in range(batch_size):\n",
        "    for i, (xi, yi) in enumerate(train_loader):\n",
        "        if i == 1:\n",
        "            break\n",
        "        xi,yi=xi.cuda(),yi.cuda()\n",
        "        #temp_x0, temp_y0 = x0[index], y0[index]\n",
        "        temp_x0 = x0 \n",
        "        #temp_x0 = temp_x0.expand(100,1,28,28)\n",
        "            \n",
        "        #b_target = torch.LongTensor([target[index]]).expand(100).cuda()\n",
        "        #b_target = torch.LongTensor([target]).expand(b_train_size).cuda()\n",
        "        b_index = ( target == yi).nonzero().squeeze()\n",
        "        if len(b_index.size()) == 0:\n",
        "            continue\n",
        "        xi = xi[b_index]\n",
        "        #b_target = b_target[b_index]\n",
        "        temp_x0 = temp_x0.expand(xi.size()).cuda()\n",
        "        theta = xi - temp_x0\n",
        "        initial_lbd = torch.norm(torch.norm(torch.norm(theta,2,1),2,1),2,1)\n",
        "        initial_lbd = initial_lbd.unsqueeze(1).unsqueeze(2).expand(xi.size()[0],dim1,dim3).unsqueeze(3).expand(xi.size()[0],dim1,dim3,dim3)\n",
        "        theta /= initial_lbd\n",
        "        lbd, query_count = initial_fine_grained_binary_search_targeted(model, temp_x0, target, theta, initial_lbd)\n",
        "        #print(lbd)    \n",
        "        best_lbd, best_index = torch.min(lbd,0)\n",
        "        #print(best_lbd)\n",
        "        best_theta = theta[best_index]\n",
        "        if best_lbd[0] < b_best_lbd:\n",
        "            \n",
        "            #print(model.predict(x0.cuda()+best_lbd*best_theta))\n",
        "            b_best_lbd = best_lbd[0]\n",
        "            b_best_theta = best_theta.clone()\n",
        "            print(\"--------> Found g() %.4f\" %b_best_lbd)\n",
        "\n",
        "    best_theta, g_theta = b_best_theta.cpu(), b_best_lbd\n",
        "     \n",
        "    #print(model.predict(x0+g_theta*best_theta))\n",
        "    timeend = time.time()\n",
        "    print(\"==========> Found best distortion %.4f in %.4f seconds using %d queries\" % (b_best_lbd, timeend-timestart, query_count))\n",
        "\n",
        "\n",
        "    # STEP II: seach for optimal\n",
        "    timestart = time.time()\n",
        "    g1 = 1.0\n",
        "    theta, g2 = best_theta.clone(), g_theta\n",
        "    print(model.predict(x0+theta*g2))\n",
        "    opt_count = 0\n",
        "    torch.manual_seed(0)\n",
        "    for i in range(iterations):\n",
        "        #alpha = 1e-3\n",
        "        #beta = 1e-3\n",
        "        u = torch.randn(theta.size())\n",
        "        u = u/torch.norm(u)\n",
        "        g2, count = fine_grained_binary_search_local_targeted(model, x0, target, theta, initial_lbd = g2)\n",
        "        opt_count += count\n",
        "        ttt = theta+beta * u\n",
        "        ttt = ttt/torch.norm(ttt)\n",
        "        ttt = ttt.type(torch.FloatTensor)\n",
        "        g1, count = fine_grained_binary_search_local_targeted(model, x0, target, ttt, initial_lbd = g2)\n",
        "        opt_count += count\n",
        "        temp_output = model.predict(x0+g2*theta)\n",
        "        if (i+1)%100 == 0:\n",
        "            print(\"Iteration %3d: g(theta + beta*u) = %.4f g(theta) = %.4f distortion %.4f num_queries %d alpha %.5f beta %.5f output %d\" % (i+1, g1, g2, g2, opt_count, alpha, beta, temp_output))\n",
        "        #if (i+1)%500 ==0:\n",
        "        #    alpha = alpha*2 \n",
        "\n",
        "        gradient = (g1-g2)/torch.norm(ttt-theta) * u\n",
        "        temp_theta = theta - alpha*gradient\n",
        "        temp_theta /= torch.norm(temp_theta)\n",
        "        g3, count = fine_grained_binary_search_local_targeted(model, x0, target, temp_theta, initial_lbd = g2)\n",
        "        if g3 > g1:\n",
        "            #print(\"aa\")\n",
        "            theta = ttt\n",
        "            #print(fine_grained_binary_search_targeted(model, x0, target, ttt, initial_lbd = g2))\n",
        "        else:\n",
        "            if g3>g2:\n",
        "                theta.sub_(o_alpha*gradient)\n",
        "            else:\n",
        "                theta.sub_(alpha*gradient)\n",
        "            theta /= torch.norm(theta)\n",
        "\n",
        "    g2, count = fine_grained_binary_search_local_targeted(model, x0, target, theta, initial_lbd = g2)\n",
        "    #distorch = torch.norm(g2*theta)\n",
        "    out_target = model.predict(x0 + g2*theta)  # should be the target\n",
        "    timeend = time.time()\n",
        "    print(\"\\nAdversarial Example Targeted %d Found Successfully: distortion %.4f target %d queries %d alpha %.5f beta %.5f \\nTime: %.4f seconds\" % (target, g2, out_target, query_count + opt_count, alpha, beta, timeend-timestart))\n",
        "    return x0 + g2*theta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'target' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [213], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m attack_targeted(model, test_loader, x, y_true, target, alpha \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m, beta \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m, iterations \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m, batch_size \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'target' is not defined"
          ]
        }
      ],
      "source": [
        "attack_targeted(model, test_loader, x, y_true, target, alpha = 0.1, beta = 0.001, iterations = 1000, batch_size = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "output with shape [1, 1, 32, 32] doesn't match the broadcast shape [1, 3, 32, 32]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [156], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cw0_attack_adversarial \u001b[39m=\u001b[39m cw0_attack\u001b[39m.\u001b[39;49mgenerate(X)\n\u001b[1;32m      2\u001b[0m cw2_attack_adversarial \u001b[39m=\u001b[39m cw2_attack\u001b[39m.\u001b[39mgenerate(X)\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/art/attacks/evasion/carlini.py:970\u001b[0m, in \u001b[0;36mCarliniL0Method.generate\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39m# No labels provided, use model prediction as correct class\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     y \u001b[39m=\u001b[39m get_labels_np_array(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimator\u001b[39m.\u001b[39;49mpredict(x, batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size))\n\u001b[1;32m    972\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator\u001b[39m.\u001b[39mnb_classes \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    973\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    974\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis attack has not yet been tested for binary classification with a single output classifier.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    975\u001b[0m     )\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/art/estimators/classification/classifier.py:74\u001b[0m, in \u001b[0;36mInputFilter.__init__.<locals>.make_replacement.<locals>.replacement_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     73\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(lst)\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m fdict[func_name](\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/art/estimators/classification/pytorch.py:321\u001b[0m, in \u001b[0;36mPyTorchClassifier.predict\u001b[0;34m(self, x, batch_size, training_mode, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m begin, end \u001b[39m=\u001b[39m (\n\u001b[1;32m    316\u001b[0m     m \u001b[39m*\u001b[39m batch_size,\n\u001b[1;32m    317\u001b[0m     \u001b[39mmin\u001b[39m((m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m batch_size, x_preprocessed\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]),\n\u001b[1;32m    318\u001b[0m )\n\u001b[1;32m    320\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 321\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(torch\u001b[39m.\u001b[39;49mfrom_numpy(x_preprocessed[begin:end])\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device))\n\u001b[1;32m    322\u001b[0m output \u001b[39m=\u001b[39m model_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    323\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/art/estimators/classification/pytorch.py:1128\u001b[0m, in \u001b[0;36mPyTorchClassifier._make_model_wrapper.<locals>.ModelWrapper.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1125\u001b[0m         result\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m   1127\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m-> 1128\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(x)\n\u001b[1;32m   1129\u001b[0m     result\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m   1131\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# pragma: no cover\u001b[39;00m\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn [100], line 14\u001b[0m, in \u001b[0;36mWrappedModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(x, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Shape of x is now [1, 3, 32, 32]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# Normalize the input\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m x_normalized \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalize(x)\n\u001b[1;32m     15\u001b[0m \u001b[39m# Return only the logits\u001b[39;00m\n\u001b[1;32m     16\u001b[0m logits, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrained_model(x_normalized)\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/torchvision/transforms/transforms.py:270\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    263\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/torchvision/transforms/functional.py:360\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    358\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 360\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mnormalize(tensor, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, inplace\u001b[39m=\u001b[39;49minplace)\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:940\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[39mif\u001b[39;00m std\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    939\u001b[0m     std \u001b[39m=\u001b[39m std\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 940\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49msub_(mean)\u001b[39m.\u001b[39mdiv_(std)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: output with shape [1, 1, 32, 32] doesn't match the broadcast shape [1, 3, 32, 32]"
          ]
        }
      ],
      "source": [
        "\n",
        "cw0_attack_adversarial = cw0_attack.generate(X)\n",
        "cw2_attack_adversarial = cw2_attack.generate(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(net, inputs):\n",
        "    \"\"\"\n",
        "    Predict labels. The cuda state of `net` decides that of the returned\n",
        "    prediction tensor.\n",
        "\n",
        "    :param net: the network\n",
        "    :param inputs: the input tensor (non Variable), of dimension [B x C x W x H]\n",
        "    :return: prediction tensor (LongTensor), of dimension [B]\n",
        "    \"\"\"\n",
        "    inputs = (net, inputs)[0]\n",
        "    inputs_var = Variable(inputs)\n",
        "    outputs_var = net(inputs_var)\n",
        "    predictions = torch.max(outputs_var.data, dim=1)[1]\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from operator import methodcaller\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def get_cuda_state(obj):\n",
        "    \"\"\"\n",
        "    Get cuda state of any object.\n",
        "\n",
        "    :param obj: an object (a tensor or an `torch.nn.Module`)\n",
        "    :raise TypeError:\n",
        "    :return: True if the object or the parameter set of the object\n",
        "             is on GPU\n",
        "    \"\"\"\n",
        "    if isinstance(obj, nn.Module):\n",
        "        try:\n",
        "            return next(obj.parameters()).is_cuda\n",
        "        except StopIteration:\n",
        "            return None\n",
        "    elif hasattr(obj, 'is_cuda'):\n",
        "        return obj.is_cuda\n",
        "    else:\n",
        "        raise TypeError('unrecognized type ({}) in args'.format(type(obj)))\n",
        "\n",
        "\n",
        "def is_cuda_consistent(*args):\n",
        "    \"\"\"\n",
        "    See if the cuda states are consistent among variables (of type either\n",
        "    tensors or torch.autograd.Variable). For example,\n",
        "\n",
        "        import torch\n",
        "        from torch.autograd import Variable\n",
        "        import torch.nn as nn\n",
        "\n",
        "        net = nn.Linear(512, 10)\n",
        "        tensor = torch.rand(10, 10).cuda()\n",
        "        assert not is_cuda_consistent(net=net, tensor=tensor)\n",
        "\n",
        "    :param args: the variables to test\n",
        "    :return: True if len(args) == 0 or the cuda states of all elements in args\n",
        "             are consistent; False otherwise\n",
        "    \"\"\"\n",
        "    result = dict()\n",
        "    for v in args:\n",
        "        cur_cuda_state = get_cuda_state(v)\n",
        "        cuda_state = result.get('cuda', cur_cuda_state)\n",
        "        if cur_cuda_state is not cuda_state:\n",
        "            return False\n",
        "        result['cuda'] = cur_cuda_state\n",
        "    return True\n",
        "\n",
        "def make_cuda_consistent(refobj, *args):\n",
        "    \"\"\"\n",
        "    Attempt to make the cuda states of args consistent with that of ``refobj``.\n",
        "    If any element of args is a Variable and the cuda state of the element is\n",
        "    inconsistent with ``refobj``, raise ValueError, since changing the cuda state\n",
        "    of a Variable involves rewrapping it in a new Variable, which changes the\n",
        "    semantics of the code.\n",
        "\n",
        "    :param refobj: either the referential object or the cuda state of the\n",
        "           referential object\n",
        "    :param args: the variables to test\n",
        "    :return: tuple of the same data as ``args`` but on the same device as\n",
        "             ``refobj``\n",
        "    \"\"\"\n",
        "    ref_cuda_state = refobj if type(refobj) is bool else get_cuda_state(refobj)\n",
        "    if ref_cuda_state is None:\n",
        "        raise ValueError('cannot determine the cuda state of `refobj` ({})'\n",
        "                .format(refobj))\n",
        "    move_to_device = methodcaller('cuda' if ref_cuda_state else 'cpu')\n",
        "\n",
        "    result_args = list()\n",
        "    for v in args:\n",
        "        cuda_state = get_cuda_state(v)\n",
        "        if cuda_state != ref_cuda_state:\n",
        "            if isinstance(v, Variable):\n",
        "                raise ValueError('cannot change cuda state of a Variable')\n",
        "            elif isinstance(v, nn.Module):\n",
        "                move_to_device(v)\n",
        "            else:\n",
        "                v = move_to_device(v)\n",
        "        result_args.append(v)\n",
        "    return tuple(result_args)\n",
        "\n",
        "def predict(net, inputs):\n",
        "    \"\"\"\n",
        "    Predict labels. The cuda state of `net` decides that of the returned\n",
        "    prediction tensor.\n",
        "\n",
        "    :param net: the network\n",
        "    :param inputs: the input tensor (non Variable), of dimension [B x C x W x H]\n",
        "    :return: prediction tensor (LongTensor), of dimension [B]\n",
        "    \"\"\"\n",
        "    inputs = make_cuda_consistent(net, inputs)[0]\n",
        "    inputs_var = Variable(inputs)\n",
        "    outputs_var = net(inputs_var)\n",
        "    predictions = torch.max(outputs_var.data, dim=1)[1]\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Carlini-Wagner attack (http://arxiv.org/abs/1608.04644).\n",
        "\n",
        "Referential implementation:\n",
        "\n",
        "- https://github.com/carlini/nn_robust_attacks.git (the original implementation)\n",
        "- https://github.com/rwightman/pytorch-nips2017-attack-example.git\n",
        "\"\"\"\n",
        "import operator as op\n",
        "from typing import Union, Tuple\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def _var2numpy(var):\n",
        "    \"\"\"\n",
        "    Make Variable to numpy array. No transposition will be made.\n",
        "\n",
        "    :param var: Variable instance on whatever device\n",
        "    :type var: Variable\n",
        "    :return: the corresponding numpy array\n",
        "    :rtype: np.ndarray\n",
        "    \"\"\"\n",
        "    return var.data.cpu().numpy()\n",
        "\n",
        "\n",
        "def atanh(x, eps=1e-6):\n",
        "    \"\"\"\n",
        "    The inverse hyperbolic tangent function, missing in pytorch.\n",
        "\n",
        "    :param x: a tensor or a Variable\n",
        "    :param eps: used to enhance numeric stability\n",
        "    :return: :math:`\\\\tanh^{-1}{x}`, of the same type as ``x``\n",
        "    \"\"\"\n",
        "    x = x * (1 - eps)\n",
        "    return 0.5 * torch.log((1.0 + x) / (1.0 - x))\n",
        "\n",
        "def to_tanh_space(x, box):\n",
        "    # type: (Union[Variable, torch.FloatTensor], Tuple[float, float]) -> Union[Variable, torch.FloatTensor]\n",
        "    \"\"\"\n",
        "    Convert a batch of tensors to tanh-space. This method complements the\n",
        "    implementation of the change-of-variable trick in terms of tanh.\n",
        "\n",
        "    :param x: the batch of tensors, of dimension [B x C x H x W]\n",
        "    :param box: a tuple of lower bound and upper bound of the box constraint\n",
        "    :return: the batch of tensors in tanh-space, of the same dimension;\n",
        "             the returned tensor is on the same device as ``x``\n",
        "    \"\"\"\n",
        "    _box_mul = (box[1] - box[0]) * 0.5\n",
        "    _box_plus = (box[1] + box[0]) * 0.5\n",
        "    return atanh((x - _box_plus) / _box_mul)\n",
        "\n",
        "def from_tanh_space(x, box):\n",
        "    # type: (Union[Variable, torch.FloatTensor], Tuple[float, float]) -> Union[Variable, torch.FloatTensor]\n",
        "    \"\"\"\n",
        "    Convert a batch of tensors from tanh-space to oridinary image space.\n",
        "    This method complements the implementation of the change-of-variable trick\n",
        "    in terms of tanh.\n",
        "\n",
        "    :param x: the batch of tensors, of dimension [B x C x H x W]\n",
        "    :param box: a tuple of lower bound and upper bound of the box constraint\n",
        "    :return: the batch of tensors in ordinary image space, of the same\n",
        "             dimension; the returned tensor is on the same device as ``x``\n",
        "    \"\"\"\n",
        "    _box_mul = (box[1] - box[0]) * 0.5\n",
        "    _box_plus = (box[1] + box[0]) * 0.5\n",
        "    return torch.tanh(x) * _box_mul + _box_plus\n",
        "\n",
        "\n",
        "class L2Adversary(object):\n",
        "    \"\"\"\n",
        "    The L2 attack adversary. To enforce the box constraint, the\n",
        "    change-of-variable trick using tanh-space is adopted.\n",
        "\n",
        "    The loss function to optimize:\n",
        "\n",
        "    .. math::\n",
        "        \\\\|\\\\delta\\\\|_2^2 + c \\\\cdot f(x + \\\\delta)\n",
        "\n",
        "    where :math:`f` is defined as\n",
        "\n",
        "    .. math::\n",
        "        f(x') = \\\\max\\\\{0, (\\\\max_{i \\\\ne t}{Z(x')_i} - Z(x')_t) \\\\cdot \\\\tau + \\\\kappa\\\\}\n",
        "\n",
        "    where :math:`\\\\tau` is :math:`+1` if the adversary performs targeted attack;\n",
        "    otherwise it's :math:`-1`.\n",
        "\n",
        "    Usage::\n",
        "\n",
        "        attacker = L2Adversary()\n",
        "        # inputs: a batch of input tensors\n",
        "        # targets: a batch of attack targets\n",
        "        # model: the model to attack\n",
        "        advx = attacker(model, inputs, targets)\n",
        "\n",
        "\n",
        "    The change-of-variable trick\n",
        "    ++++++++++++++++++++++++++++\n",
        "\n",
        "    Let :math:`a` be a proper affine transformation.\n",
        "\n",
        "    1. Given input :math:`x` in image space, map :math:`x` to \"tanh-space\" by\n",
        "\n",
        "    .. math:: \\\\hat{x} = \\\\tanh^{-1}(a^{-1}(x))\n",
        "\n",
        "    2. Optimize an adversarial perturbation :math:`m` without constraint in the\n",
        "    \"tanh-space\", yielding an adversarial example :math:`w = \\\\hat{x} + m`; and\n",
        "\n",
        "    3. Map :math:`w` back to the same image space as the one where :math:`x`\n",
        "    resides:\n",
        "\n",
        "    .. math::\n",
        "        x' = a(\\\\tanh(w))\n",
        "\n",
        "    where :math:`x'` is the adversarial example, and :math:`\\\\delta = x' - x`\n",
        "    is the adversarial perturbation.\n",
        "\n",
        "    Since the composition of affine transformation and hyperbolic tangent is\n",
        "    strictly monotonic, $\\\\delta = 0$ if and only if $m = 0$.\n",
        "\n",
        "    Symbols used in docstring\n",
        "    +++++++++++++++++++++++++\n",
        "\n",
        "    - ``B``: the batch size\n",
        "    - ``C``: the number of channels\n",
        "    - ``H``: the height\n",
        "    - ``W``: the width\n",
        "    - ``M``: the number of classes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, targeted=True, confidence=0.0, c_range=(1e-3, 1e10),\n",
        "                 search_steps=5, max_steps=1000, abort_early=True,\n",
        "                 box=(-1., 1.), optimizer_lr=1e-2, init_rand=False):\n",
        "        \"\"\"\n",
        "        :param targeted: ``True`` to perform targeted attack in ``self.run``\n",
        "               method\n",
        "        :type targeted: bool\n",
        "        :param confidence: the confidence constant, i.e. the $\\\\kappa$ in paper\n",
        "        :type confidence: float\n",
        "        :param c_range: the search range of the constant :math:`c`; should be a\n",
        "               tuple of form (lower_bound, upper_bound)\n",
        "        :type c_range: Tuple[float, float]\n",
        "        :param search_steps: the number of steps to perform binary search of\n",
        "               the constant :math:`c` over ``c_range``\n",
        "        :type search_steps: int\n",
        "        :param max_steps: the maximum number of optimization steps for each\n",
        "               constant :math:`c`\n",
        "        :type max_steps: int\n",
        "        :param abort_early: ``True`` to abort early in process of searching for\n",
        "               :math:`c` when the loss virtually stops increasing\n",
        "        :type abort_early: bool\n",
        "        :param box: a tuple of lower bound and upper bound of the box\n",
        "        :type box: Tuple[float, float]\n",
        "        :param optimizer_lr: the base learning rate of the Adam optimizer used\n",
        "               over the adversarial perturbation in clipped space\n",
        "        :type optimizer_lr: float\n",
        "        :param init_rand: ``True`` to initialize perturbation to small Gaussian;\n",
        "               False is consistent with the original paper, where the\n",
        "               perturbation is initialized to zero\n",
        "        :type init_rand: bool\n",
        "        :rtype: None\n",
        "\n",
        "        Why to make ``box`` default to (-1., 1.) rather than (0., 1.)? TL;DR the\n",
        "        domain of the problem in pytorch is [-1, 1] instead of [0, 1].\n",
        "        According to Xiang Xu (samxucmu@gmail.com)::\n",
        "\n",
        "        > The reason is that in pytorch a transformation is applied first\n",
        "        > before getting the input from the data loader. So image in range [0,1]\n",
        "        > will subtract some mean and divide by std. The normalized input image\n",
        "        > will now be in range [-1,1]. For this implementation, clipping is\n",
        "        > actually performed on the image after normalization, not on the\n",
        "        > original image.\n",
        "\n",
        "        Why to ``optimizer_lr`` default to 1e-2? The optimizer used in Carlini's\n",
        "        code adopts 1e-2. In another pytorch implementation\n",
        "        (https://github.com/rwightman/pytorch-nips2017-attack-example.git),\n",
        "        though, the learning rate is set to 5e-4.\n",
        "        \"\"\"\n",
        "        if len(c_range) != 2:\n",
        "            raise TypeError('c_range ({}) should be of form '\n",
        "                            'tuple([lower_bound, upper_bound])'\n",
        "                            .format(c_range))\n",
        "        if c_range[0] >= c_range[1]:\n",
        "            raise ValueError('c_range lower bound ({}) is expected to be less '\n",
        "                             'than c_range upper bound ({})'.format(*c_range))\n",
        "        if len(box) != 2:\n",
        "            raise TypeError('box ({}) should be of form '\n",
        "                            'tuple([lower_bound, upper_bound])'\n",
        "                            .format(box))\n",
        "        if box[0] >= box[1]:\n",
        "            raise ValueError('box lower bound ({}) is expected to be less than '\n",
        "                             'box upper bound ({})'.format(*box))\n",
        "        self.targeted = targeted\n",
        "        self.confidence = float(confidence)\n",
        "        self.c_range = (float(c_range[0]), float(c_range[1]))\n",
        "        self.binary_search_steps = search_steps\n",
        "        self.max_steps = max_steps\n",
        "        self.abort_early = abort_early\n",
        "        self.ae_tol = 1e-4  # tolerance of early abort\n",
        "        self.box = tuple(map(float, box))  # type: Tuple[float, float]\n",
        "        self.optimizer_lr = optimizer_lr\n",
        "\n",
        "        # `self.init_rand` is not in Carlini's code, it's an attempt in the\n",
        "        # referencing pytorch implementation to improve the quality of attacks.\n",
        "        self.init_rand = init_rand\n",
        "\n",
        "        # Since the larger the `scale_const` is, the more likely a successful\n",
        "        # attack can be found, `self.repeat` guarantees at least attempt the\n",
        "        # largest scale_const once. Moreover, since the optimal criterion is the\n",
        "        # L2 norm of the attack, and the larger `scale_const` is, the larger\n",
        "        # the L2 norm is, thus less optimal, the last attempt at the largest\n",
        "        # `scale_const` won't ruin the optimum ever found.\n",
        "        self.repeat = (self.binary_search_steps >= 10)\n",
        "\n",
        "    def __call__(self, model, inputs, targets, to_numpy=True):\n",
        "        \"\"\"\n",
        "        Produce adversarial examples for ``inputs``.\n",
        "\n",
        "        :param model: the model to attack\n",
        "        :type model: nn.Module\n",
        "        :param inputs: the original images tensor, of dimension [B x C x H x W].\n",
        "               ``inputs`` can be on either CPU or GPU, but it will eventually be\n",
        "               moved to the same device as the one the parameters of ``model``\n",
        "               reside\n",
        "        :type inputs: torch.FloatTensor\n",
        "        :param targets: the original image labels, or the attack targets, of\n",
        "               dimension [B]. If ``self.targeted`` is ``True``, then ``targets``\n",
        "               is treated as the attack targets, otherwise the labels.\n",
        "               ``targets`` can be on either CPU or GPU, but it will eventually\n",
        "               be moved to the same device as the one the parameters of\n",
        "               ``model`` reside\n",
        "        :type targets: torch.LongTensor\n",
        "        :param to_numpy: True to return an `np.ndarray`, otherwise,\n",
        "               `torch.FloatTensor`\n",
        "        :type to_numpy: bool\n",
        "        :return: the adversarial examples on CPU, of dimension [B x C x H x W]\n",
        "        \"\"\"\n",
        "        # sanity check\n",
        "        #assert isinstance(model, nn.Module)\n",
        "        #assert len(inputs.size()) == 4\n",
        "        #assert len(targets.size()) == 1\n",
        "\n",
        "        # get a copy of targets in numpy before moving to GPU, used when doing\n",
        "        # the binary search on `scale_const`\n",
        "        targets_np = targets.clone().cpu().numpy()  # type: np.ndarray\n",
        "\n",
        "        # the type annotations here are used only for type hinting and do\n",
        "        # not indicate the actual type (cuda or cpu); same applies to all codes\n",
        "        # below\n",
        "        inputs = (model, inputs)[0]  # type: torch.FloatTensor\n",
        "        targets = (model, targets)[0]  # type: torch.FloatTensor\n",
        "\n",
        "        # run the model a little bit to get the `num_classes`\n",
        "        num_classes = model(Variable(inputs[0][None, :], requires_grad=False)).size(1)  # type: int\n",
        "        batch_size = inputs.size(0)  # type: int\n",
        "\n",
        "        # `lower_bounds_np`, `upper_bounds_np` and `scale_consts_np` are used\n",
        "        # for binary search of each `scale_const` in the batch. The element-wise\n",
        "        # inquality holds: lower_bounds_np < scale_consts_np <= upper_bounds_np\n",
        "        lower_bounds_np = np.zeros(batch_size)\n",
        "        upper_bounds_np = np.ones(batch_size) * self.c_range[1]\n",
        "        scale_consts_np = np.ones(batch_size) * self.c_range[0]\n",
        "\n",
        "        # Optimal attack to be found.\n",
        "        # The three \"placeholders\" are defined as:\n",
        "        # - `o_best_l2`: the least L2 norms\n",
        "        # - `o_best_l2_ppred`: the perturbed predictions made by the adversarial\n",
        "        #    perturbations with the least L2 norms\n",
        "        # - `o_best_advx`: the underlying adversarial example of\n",
        "        #   `o_best_l2_ppred`\n",
        "        o_best_l2 = np.ones(batch_size) * np.inf\n",
        "        o_best_l2_ppred = -np.ones(batch_size)\n",
        "        o_best_advx = inputs.clone().cpu().numpy()  # type: np.ndarray\n",
        "\n",
        "        # convert `inputs` to tanh-space\n",
        "        inputs_tanh = self._to_tanh_space(inputs)  # type: torch.FloatTensor\n",
        "        inputs_tanh_var = Variable(inputs_tanh, requires_grad=False)\n",
        "\n",
        "        # the one-hot encoding of `targets`\n",
        "        targets_oh = torch.zeros(targets.size() + (num_classes,))  # type: torch.FloatTensor\n",
        "        targets_oh = (model, targets_oh)[0]\n",
        "        targets_oh.scatter_(1, targets.unsqueeze(1), 1.0)\n",
        "        targets_oh_var = Variable(targets_oh, requires_grad=False)\n",
        "\n",
        "        # the perturbation variable to optimize.\n",
        "        # `pert_tanh` is essentially the adversarial perturbation in tanh-space.\n",
        "        # In Carlini's code it's denoted as `modifier`\n",
        "        pert_tanh = torch.zeros(inputs.size())  # type: torch.FloatTensor\n",
        "        if self.init_rand:\n",
        "            nn.init.normal(pert_tanh, mean=0, std=1e-3)\n",
        "        pert_tanh = (model, pert_tanh)[0]\n",
        "        pert_tanh_var = Variable(pert_tanh, requires_grad=True)\n",
        "\n",
        "        optimizer = optim.Adam([pert_tanh_var], lr=self.optimizer_lr)\n",
        "        for sstep in range(self.binary_search_steps):\n",
        "            if self.repeat and sstep == self.binary_search_steps - 1:\n",
        "                scale_consts_np = upper_bounds_np\n",
        "            scale_consts = torch.from_numpy(np.copy(scale_consts_np)).float()  # type: torch.FloatTensor\n",
        "            scale_consts = (model, scale_consts)[0]\n",
        "            scale_consts_var = Variable(scale_consts, requires_grad=False)\n",
        "            print('Using scale consts:', list(scale_consts_np))\n",
        "\n",
        "            # the minimum L2 norms of perturbations found during optimization\n",
        "            best_l2 = np.ones(batch_size) * np.inf\n",
        "            # the perturbed predictions corresponding to `best_l2`, to be used\n",
        "            # in binary search of `scale_const`\n",
        "            best_l2_ppred = -np.ones(batch_size)\n",
        "            # previous (summed) batch loss, to be used in early stopping policy\n",
        "            prev_batch_loss = np.inf  # type: float\n",
        "            for optim_step in range(self.max_steps):\n",
        "                batch_loss, pert_norms_np, pert_outputs_np, advxs_np = \\\n",
        "                    self._optimize(model, optimizer, inputs_tanh_var,\n",
        "                                   pert_tanh_var, targets_oh_var,\n",
        "                                   scale_consts_var)\n",
        "                if optim_step % 10 == 0: print('batch [{}] loss: {}'.format(optim_step, batch_loss)) \n",
        "\n",
        "                if self.abort_early and not optim_step % (self.max_steps // 10):\n",
        "                    if batch_loss > prev_batch_loss * (1 - self.ae_tol):\n",
        "                        break\n",
        "                    prev_batch_loss = batch_loss\n",
        "\n",
        "                # update best attack found during optimization\n",
        "                pert_predictions_np = np.argmax(pert_outputs_np, axis=1)\n",
        "                comp_pert_predictions_np = np.argmax(\n",
        "                        self._compensate_confidence(pert_outputs_np,\n",
        "                                                    targets_np),\n",
        "                        axis=1)\n",
        "                for i in range(batch_size):\n",
        "                    l2 = pert_norms_np[i]\n",
        "                    cppred = comp_pert_predictions_np[i]\n",
        "                    ppred = pert_predictions_np[i]\n",
        "                    tlabel = targets_np[i]\n",
        "                    ax = advxs_np[i]\n",
        "                    if self._attack_successful(cppred, tlabel):\n",
        "                        assert cppred == ppred\n",
        "                        if l2 < best_l2[i]:\n",
        "                            best_l2[i] = l2\n",
        "                            best_l2_ppred[i] = ppred\n",
        "                        if l2 < o_best_l2[i]:\n",
        "                            o_best_l2[i] = l2\n",
        "                            o_best_l2_ppred[i] = ppred\n",
        "                            o_best_advx[i] = ax\n",
        "\n",
        "            # binary search of `scale_const`\n",
        "            for i in range(batch_size):\n",
        "                tlabel = targets_np[i]\n",
        "                assert best_l2_ppred[i] == -1 or \\\n",
        "                       self._attack_successful(best_l2_ppred[i], tlabel)\n",
        "                assert o_best_l2_ppred[i] == -1 or \\\n",
        "                       self._attack_successful(o_best_l2_ppred[i], tlabel)\n",
        "                if best_l2_ppred[i] != -1:\n",
        "                    # successful; attempt to lower `scale_const` by halving it\n",
        "                    if scale_consts_np[i] < upper_bounds_np[i]:\n",
        "                        upper_bounds_np[i] = scale_consts_np[i]\n",
        "                    # `upper_bounds_np[i] == c_range[1]` implies no solution\n",
        "                    # found, i.e. upper_bounds_np[i] has never been updated by\n",
        "                    # scale_consts_np[i] until\n",
        "                    # `scale_consts_np[i] > 0.1 * c_range[1]`\n",
        "                    if upper_bounds_np[i] < self.c_range[1] * 0.1:\n",
        "                        scale_consts_np[i] = (lower_bounds_np[i] + upper_bounds_np[i]) / 2\n",
        "                else:\n",
        "                    # failure; multiply `scale_const` by ten if no solution\n",
        "                    # found; otherwise do binary search\n",
        "                    if scale_consts_np[i] > lower_bounds_np[i]:\n",
        "                        lower_bounds_np[i] = scale_consts_np[i]\n",
        "                    if upper_bounds_np[i] < self.c_range[1] * 0.1:\n",
        "                        scale_consts_np[i] = (lower_bounds_np[i] + upper_bounds_np[i]) / 2\n",
        "                    else:\n",
        "                        scale_consts_np[i] *= 10\n",
        "\n",
        "        if not to_numpy:\n",
        "            o_best_advx = torch.from_numpy(o_best_advx).float()\n",
        "        return o_best_advx\n",
        "\n",
        "    def _optimize(self, model, optimizer, inputs_tanh_var, pert_tanh_var,\n",
        "                  targets_oh_var, c_var):\n",
        "        \"\"\"\n",
        "        Optimize for one step.\n",
        "\n",
        "        :param model: the model to attack\n",
        "        :type model: nn.Module\n",
        "        :param optimizer: the Adam optimizer to optimize ``modifier_var``\n",
        "        :type optimizer: optim.Adam\n",
        "        :param inputs_tanh_var: the input images in tanh-space\n",
        "        :type inputs_tanh_var: Variable\n",
        "        :param pert_tanh_var: the perturbation to optimize in tanh-space,\n",
        "               ``pert_tanh_var.requires_grad`` flag must be set to True\n",
        "        :type pert_tanh_var: Variable\n",
        "        :param targets_oh_var: the one-hot encoded target tensor (the attack\n",
        "               targets if self.targeted else image labels)\n",
        "        :type targets_oh_var: Variable\n",
        "        :param c_var: the constant :math:`c` for each perturbation of a batch,\n",
        "               a Variable of FloatTensor of dimension [B]\n",
        "        :type c_var: Variable\n",
        "        :return: the batch loss, squared L2-norm of adversarial perturbations\n",
        "                 (of dimension [B]), the perturbed activations (of dimension\n",
        "                 [B]), the adversarial examples (of dimension [B x C x H x W])\n",
        "        \"\"\"\n",
        "        # the adversarial examples in the image space\n",
        "        # of dimension [B x C x H x W]\n",
        "        advxs_var = self._from_tanh_space(inputs_tanh_var + pert_tanh_var)  # type: Variable\n",
        "        # the perturbed activation before softmax\n",
        "        pert_outputs_var = model(advxs_var)  # type: Variable\n",
        "        # the original inputs\n",
        "        inputs_var = self._from_tanh_space(inputs_tanh_var)  # type: Variable\n",
        "\n",
        "        perts_norm_var = torch.pow(advxs_var - inputs_var, 2)\n",
        "        perts_norm_var = torch.sum(perts_norm_var.view(\n",
        "                perts_norm_var.size(0), -1), 1)\n",
        "\n",
        "        # In Carlini's code, `target_activ_var` is called `real`.\n",
        "        # It should be a Variable of tensor of dimension [B], such that the\n",
        "        # `target_activ_var[i]` is the final activation (right before softmax)\n",
        "        # of the $t$th class, where $t$ is the attack target or the image label\n",
        "        #\n",
        "        # noinspection PyArgumentList\n",
        "        target_activ_var = torch.sum(targets_oh_var * pert_outputs_var, 1)\n",
        "        inf = 1e4  # sadly pytorch does not work with np.inf;\n",
        "                   # 1e4 is also used in Carlini's code\n",
        "        # In Carlini's code, `maxother_activ_var` is called `other`.\n",
        "        # It should be a Variable of tensor of dimension [B], such that the\n",
        "        # `maxother_activ_var[i]` is the maximum final activation of all classes\n",
        "        # other than class $t$, where $t$ is the attack target or the image\n",
        "        # label.\n",
        "        #\n",
        "        # The assertion here ensures (sufficiently yet not necessarily) the\n",
        "        # assumption behind the trick to get `maxother_activ_var` holds, that\n",
        "        # $\\max_{i \\ne t}{o_i} \\ge -\\text{_inf}$, where $t$ is the target and\n",
        "        # $o_i$ the $i$th element along axis=1 of `pert_outputs_var`.\n",
        "        #\n",
        "        # noinspection PyArgumentList\n",
        "        assert (pert_outputs_var.max(1)[0] >= -inf).all(), 'assumption failed'\n",
        "        # noinspection PyArgumentList\n",
        "        maxother_activ_var = torch.max(((1 - targets_oh_var) * pert_outputs_var\n",
        "                                        - targets_oh_var * inf), 1)[0]\n",
        "\n",
        "        # Compute $f(x')$, where $x'$ is the adversarial example in image space.\n",
        "        # The result `f_var` should be of dimension [B]\n",
        "        if self.targeted:\n",
        "            # if targeted, optimize to make `target_activ_var` larger than\n",
        "            # `maxother_activ_var` by `self.confidence`\n",
        "            #\n",
        "            # noinspection PyArgumentList\n",
        "            f_var = torch.clamp(maxother_activ_var - target_activ_var\n",
        "                                + self.confidence, min=0.0)\n",
        "        else:\n",
        "            # if not targeted, optimize to make `maxother_activ_var` larger than\n",
        "            # `target_activ_var` (the ground truth image labels) by\n",
        "            # `self.confidence`\n",
        "            #\n",
        "            # noinspection PyArgumentList\n",
        "            f_var = torch.clamp(target_activ_var - maxother_activ_var\n",
        "                                + self.confidence, min=0.0)\n",
        "        # the total loss of current batch, should be of dimension [1]\n",
        "        batch_loss_var = torch.sum(perts_norm_var + c_var * f_var)  # type: Variable\n",
        "\n",
        "        # Do optimization for one step\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss_var.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Make some records in python/numpy on CPU\n",
        "        batch_loss = batch_loss_var.data[0]  # type: float\n",
        "        pert_norms_np = _var2numpy(perts_norm_var)\n",
        "        pert_outputs_np = _var2numpy(pert_outputs_var)\n",
        "        advxs_np = _var2numpy(advxs_var)\n",
        "        return batch_loss, pert_norms_np, pert_outputs_np, advxs_np\n",
        "\n",
        "    def _attack_successful(self, prediction, target):\n",
        "        \"\"\"\n",
        "        See whether the underlying attack is successful.\n",
        "\n",
        "        :param prediction: the prediction of the model on an input\n",
        "        :type prediction: int\n",
        "        :param target: either the attack target or the ground-truth image label\n",
        "        :type target: int\n",
        "        :return: ``True`` if the attack is successful\n",
        "        :rtype: bool\n",
        "        \"\"\"\n",
        "        if self.targeted:\n",
        "            return prediction == target\n",
        "        else:\n",
        "            return prediction != target\n",
        "\n",
        "    # noinspection PyUnresolvedReferences\n",
        "    def _compensate_confidence(self, outputs, targets):\n",
        "        \"\"\"\n",
        "        Compensate for ``self.confidence`` and returns a new weighted sum\n",
        "        vector.\n",
        "\n",
        "        :param outputs: the weighted sum right before the last layer softmax\n",
        "               normalization, of dimension [B x M]\n",
        "        :type outputs: np.ndarray\n",
        "        :param targets: either the attack targets or the real image labels,\n",
        "               depending on whether or not ``self.targeted``, of dimension [B]\n",
        "        :type targets: np.ndarray\n",
        "        :return: the compensated weighted sum of dimension [B x M]\n",
        "        :rtype: np.ndarray\n",
        "        \"\"\"\n",
        "        outputs_comp = np.copy(outputs)\n",
        "        rng = np.arange(targets.shape[0])\n",
        "        if self.targeted:\n",
        "            # for each image $i$:\n",
        "            # if targeted, `outputs[i, target_onehot]` should be larger than\n",
        "            # `max(outputs[i, ~target_onehot])` by `self.confidence`\n",
        "            outputs_comp[rng, targets] -= self.confidence\n",
        "        else:\n",
        "            # for each image $i$:\n",
        "            # if not targeted, `max(outputs[i, ~target_onehot]` should be larger\n",
        "            # than `outputs[i, target_onehot]` (the ground truth image labels)\n",
        "            # by `self.confidence`\n",
        "            outputs_comp[rng, targets] += self.confidence\n",
        "        return outputs_comp\n",
        "\n",
        "    def _to_tanh_space(self, x):\n",
        "        \"\"\"\n",
        "        Convert a batch of tensors to tanh-space.\n",
        "\n",
        "        :param x: the batch of tensors, of dimension [B x C x H x W]\n",
        "        :return: the batch of tensors in tanh-space, of the same dimension\n",
        "        \"\"\"\n",
        "        return to_tanh_space(x, self.box)\n",
        "\n",
        "    def _from_tanh_space(self, x):\n",
        "        \"\"\"\n",
        "        Convert a batch of tensors from tanh-space to input space.\n",
        "\n",
        "        :param x: the batch of tensors, of dimension [B x C x H x W]\n",
        "        :return: the batch of tensors in tanh-space, of the same dimension;\n",
        "                 the returned tensor is on the same device as ``x``\n",
        "        \"\"\"\n",
        "        return from_tanh_space(x, self.box)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "mean = (0.3403, 0.3121, 0.3214)\n",
        "std = (0.2724, 0.2608, 0.2669)\n",
        "normalization = torchvision.transforms.Normalize(mean, std)\n",
        "inputs_box = (min((0 - m) / s for m, s in zip(mean, std)),\n",
        "              max((1 - m) / s for m, s in zip(mean, std))) \n",
        "\n",
        "\n",
        "adversary = L2Adversary(targeted=False,\n",
        "                           confidence=0.0,\n",
        "                           search_steps=10,\n",
        "                           box=inputs_box,\n",
        "                           optimizer_lr=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Model:\n",
        "   ...:     def __init__(self,model=None):\n",
        "   ...:         self.model = model\n",
        "   ...:     def __repr__(self):\n",
        "   ...:         return 'User(model={})'.format(self.model)\n",
        "   ...:     def __getitem__(self, key):\n",
        "   ...:         return self.__dict__[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "type object 'Model' has no attribute 'model'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs, targets \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(test_loader))\n\u001b[0;32m----> 2\u001b[0m adversarial_examples \u001b[39m=\u001b[39m adversary(Model\u001b[39m.\u001b[39;49mmodel, inputs, targets, to_numpy\u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(adversarial_examples, torch\u001b[39m.\u001b[39mFloatTensor)\n\u001b[1;32m      4\u001b[0m \u001b[39massert\u001b[39;00m adversarial_examples\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m inputs\u001b[39m.\u001b[39msize()\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'Model' has no attribute 'model'"
          ]
        }
      ],
      "source": [
        "inputs, targets = next(iter(test_loader))\n",
        "adversarial_examples = adversary(Model.model, inputs, targets, to_numpy= False)\n",
        "assert isinstance(adversarial_examples, torch.FloatTensor)\n",
        "assert adversarial_examples.size() == inputs.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "adversary = L2Adversary(targeted=True,\n",
        "                           confidence=0.0,\n",
        "                           search_steps=10,\n",
        "                           box=inputs_box,\n",
        "                           optimizer_lr=5e-4)\n",
        "\n",
        "inputs, _ = next(iter(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'LeNet5' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m attack_targets \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)) \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m----> 2\u001b[0m adversarial_examples \u001b[39m=\u001b[39m adversary(model, inputs, attack_targets, to_numpy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      3\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(adversarial_examples, torch\u001b[39m.\u001b[39mFloatTensor)\n\u001b[1;32m      4\u001b[0m \u001b[39massert\u001b[39;00m adversarial_examples\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m inputs\u001b[39m.\u001b[39msize()\n",
            "Cell \u001b[0;32mIn [40], line 254\u001b[0m, in \u001b[0;36mL2Adversary.__call__\u001b[0;34m(self, model, inputs, targets, to_numpy)\u001b[0m\n\u001b[1;32m    251\u001b[0m targets \u001b[39m=\u001b[39m (model, targets)[\u001b[39m0\u001b[39m]  \u001b[39m# type: torch.FloatTensor\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39m# run the model a little bit to get the `num_classes`\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m num_classes \u001b[39m=\u001b[39m model(Variable(inputs[\u001b[39m0\u001b[39;49m][\u001b[39mNone\u001b[39;00m, :], requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)  \u001b[39m# type: int\u001b[39;00m\n\u001b[1;32m    255\u001b[0m batch_size \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)  \u001b[39m# type: int\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m# `lower_bounds_np`, `upper_bounds_np` and `scale_consts_np` are used\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m# for binary search of each `scale_const` in the batch. The element-wise\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39m# inquality holds: lower_bounds_np < scale_consts_np <= upper_bounds_np\u001b[39;00m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'LeNet5' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "attack_targets = torch.ones(inputs.size(0)) * 3\n",
        "adversarial_examples = adversary(model, inputs, attack_targets, to_numpy=True)\n",
        "assert isinstance(adversarial_examples, torch.FloatTensor)\n",
        "assert adversarial_examples.size() == inputs.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time \n",
        "import logging \n",
        "from collections import OrderedDict \n",
        "from collections.abc import Iterable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def wrapper_method(func):\n",
        "    def wrapper_func(self, *args, **kwargs):\n",
        "        result = func(self, *args, **kwargs)\n",
        "        for atk in self.__dict__.get('_attacks').values():\n",
        "            eval(\"atk.\"+func.__name__+\"(*args, **kwargs)\")\n",
        "        return result\n",
        "    return wrapper_func\n",
        "\n",
        "\n",
        "class Attack(object):\n",
        "    r\"\"\"\n",
        "    Base class for all attacks.\n",
        "    .. note::\n",
        "        It automatically set device to the device where given model is.\n",
        "        It basically changes training mode to eval during attack process.\n",
        "        To change this, please see `set_model_training_mode`.\n",
        "    \"\"\"\n",
        "    def __init__(self, name, model):\n",
        "        r\"\"\"\n",
        "        Initializes internal attack state.\n",
        "        Arguments:\n",
        "            name (str): name of attack.\n",
        "            model (torch.nn.Module): model to attack.\n",
        "        \"\"\"\n",
        "\n",
        "        self.attack = name\n",
        "        self._attacks = OrderedDict()\n",
        "        \n",
        "        self.set_model(model)\n",
        "        self.device = next(model.parameters()).device\n",
        "        self.return_type = 'float'\n",
        "\n",
        "        # Controls attack mode.\n",
        "        self.attack_mode = 'default'\n",
        "        self.supported_mode = ['default']\n",
        "        self.targeted = False\n",
        "        self._target_map_function = None\n",
        "\n",
        "        # Controls when normalization is used.\n",
        "        self.normalization_used = None\n",
        "        self._normalization_applied = None\n",
        "        self._set_auto_normalization_used(model)\n",
        "\n",
        "        # Controls model mode during attack.\n",
        "        self._model_training = False\n",
        "        self._batchnorm_training = False\n",
        "        self._dropout_training = False\n",
        "\n",
        "    def forward(self, inputs, labels=None, *args, **kwargs):\n",
        "        r\"\"\"\n",
        "        It defines the computation performed at every call.\n",
        "        Should be overridden by all subclasses.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    @wrapper_method\n",
        "    def set_model(self, model):\n",
        "        self.model = model\n",
        "        self.model_name = str(model).split(\"(\")[0]\n",
        "\n",
        "    def get_logits(self, inputs, labels=None, *args, **kwargs):\n",
        "        if self._normalization_applied is False:\n",
        "            inputs = self.normalize(inputs)\n",
        "        logits = self.model(inputs)\n",
        "        return logits\n",
        "\n",
        "    @wrapper_method\n",
        "    def _set_normalization_applied(self, flag):\n",
        "        self._normalization_applied = flag\n",
        "    \n",
        "    @wrapper_method\n",
        "    def set_device(self, device):\n",
        "        self.device = device\n",
        "\n",
        "    @wrapper_method\n",
        "    def _set_auto_normalization_used(self, model):\n",
        "        mean = getattr(model, 'mean', None)\n",
        "        std = getattr(model, 'std', None)\n",
        "        if (mean is not None) and (std is not None):\n",
        "            if isinstance(mean, torch.Tensor):\n",
        "                mean = mean.cpu().numpy()\n",
        "            if isinstance(std, torch.Tensor):\n",
        "                std = std.cpu().numpy()\n",
        "            if (mean != 0).all() or (std != 1).all():\n",
        "                self.set_normalization_used(mean, std)\n",
        "#                 logging.info(\"Normalization automatically loaded from `model.mean` and `model.std`.\")\n",
        "\n",
        "    @wrapper_method\n",
        "    def set_normalization_used(self, mean, std):\n",
        "        self.normalization_used = {}\n",
        "        n_channels = len(mean)\n",
        "        mean = torch.tensor(mean).reshape(1, n_channels, 1, 1)\n",
        "        std = torch.tensor(std).reshape(1, n_channels, 1, 1)\n",
        "        self.normalization_used['mean'] = mean\n",
        "        self.normalization_used['std'] = std\n",
        "        self._normalization_applied = True\n",
        "\n",
        "    def normalize(self, inputs):\n",
        "        mean = self.normalization_used['mean'].to(inputs.device)\n",
        "        std = self.normalization_used['std'].to(inputs.device)\n",
        "        return (inputs - mean) / std\n",
        "\n",
        "    def inverse_normalize(self, inputs):\n",
        "        mean = self.normalization_used['mean'].to(inputs.device)\n",
        "        std = self.normalization_used['std'].to(inputs.device)\n",
        "        return inputs*std + mean\n",
        "\n",
        "    def get_mode(self):\n",
        "        r\"\"\"\n",
        "        Get attack mode.\n",
        "        \"\"\"\n",
        "        return self.attack_mode\n",
        "\n",
        "    @wrapper_method\n",
        "    def set_mode_default(self):\n",
        "        r\"\"\"\n",
        "        Set attack mode as default mode.\n",
        "        \"\"\"\n",
        "        self.attack_mode = 'default'\n",
        "        self.targeted = False\n",
        "        print(\"Attack mode is changed to 'default.'\")\n",
        "\n",
        "    @wrapper_method\n",
        "    def _set_mode_targeted(self, mode):\n",
        "        if \"targeted\" not in self.supported_mode:\n",
        "            raise ValueError(\"Targeted mode is not supported.\")\n",
        "        self.targeted = True\n",
        "        self.attack_mode = mode\n",
        "        print(\"Attack mode is changed to '%s'.\"%mode)\n",
        "\n",
        "    @wrapper_method\n",
        "    def set_mode_targeted_by_function(self, target_map_function):\n",
        "        r\"\"\"\n",
        "        Set attack mode as targeted.\n",
        "        Arguments:\n",
        "            target_map_function (function): Label mapping function.\n",
        "                e.g. lambda inputs, labels:(labels+1)%10.\n",
        "                None for using input labels as targeted labels. (Default)\n",
        "        \"\"\"\n",
        "        self._set_mode_targeted('targeted(custom)')\n",
        "        self._target_map_function = target_map_function\n",
        "\n",
        "    @wrapper_method\n",
        "    def set_mode_targeted_random(self):\n",
        "        r\"\"\"\n",
        "        Set attack mode as targeted with random labels.\n",
        "        Arguments:\n",
        "            num_classses (str): number of classes.\n",
        "        \"\"\"\n",
        "        self._set_mode_targeted('targeted(random)')\n",
        "        self._target_map_function = self.get_random_target_label\n",
        "\n",
        "    @wrapper_method\n",
        "    def set_mode_targeted_least_likely(self, kth_min=1):\n",
        "        r\"\"\"\n",
        "        Set attack mode as targeted with least likely labels.\n",
        "        Arguments:\n",
        "            kth_min (str): label with the k-th smallest probability used as target labels. (Default: 1)\n",
        "        \"\"\"\n",
        "        self._set_mode_targeted('targeted(least-likely)')\n",
        "        assert (kth_min > 0)\n",
        "        self._kth_min = kth_min\n",
        "        self._target_map_function = self.get_least_likely_label\n",
        "\n",
        "    @wrapper_method\n",
        "    def set_return_type(self, type):\n",
        "        r\"\"\"\n",
        "        Set the return type of adversarial inputs: `int` or `float`.\n",
        "        Arguments:\n",
        "            type (str): 'float' or 'int'. (Default: 'float')\n",
        "        .. note::\n",
        "            If 'int' is used for the return type, the file size of \n",
        "            adversarial inputs can be reduced (about 1/4 for CIFAR10).\n",
        "            However, if the attack originally outputs float adversarial inputs\n",
        "            (e.g. using small step-size than 1/255), it might reduce the attack\n",
        "            success rate of the attack.\n",
        "        \"\"\"\n",
        "        if type == 'float':\n",
        "            self.return_type = 'float'\n",
        "        elif type == 'int':\n",
        "            self.return_type = 'int'\n",
        "        else:\n",
        "            raise ValueError(type + \" is not a valid type. [Options: float, int]\")\n",
        "\n",
        "    def get_return_type(self):\n",
        "        r\"\"\"\n",
        "        Get the return type of adversarial inputs: `int` or `float`.\n",
        "        \"\"\"\n",
        "        return self.return_type\n",
        "\n",
        "    @wrapper_method\n",
        "    def set_model_training_mode(self, model_training=False, batchnorm_training=False, dropout_training=False):\n",
        "        r\"\"\"\n",
        "        Set training mode during attack process.\n",
        "        Arguments:\n",
        "            model_training (bool): True for using training mode for the entire model during attack process.\n",
        "            batchnorm_training (bool): True for using training mode for batchnorms during attack process.\n",
        "            dropout_training (bool): True for using training mode for dropouts during attack process.\n",
        "        .. note::\n",
        "            For RNN-based models, we cannot calculate gradients with eval mode.\n",
        "            Thus, it should be changed to the training mode during the attack.\n",
        "        \"\"\"\n",
        "        self._model_training = model_training\n",
        "        self._batchnorm_training = batchnorm_training\n",
        "        self._dropout_training = dropout_training\n",
        "\n",
        "    @wrapper_method\n",
        "    def _change_model_mode(self, given_training):\n",
        "        if self._model_training:\n",
        "            self.model.train()\n",
        "            for _, m in self.model.named_modules():\n",
        "                if not self._batchnorm_training:\n",
        "                    if 'BatchNorm' in m.__class__.__name__:\n",
        "                        m = m.eval()\n",
        "                if not self._dropout_training:\n",
        "                    if 'Dropout' in m.__class__.__name__:\n",
        "                        m = m.eval()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "    @wrapper_method\n",
        "    def _recover_model_mode(self, given_training):\n",
        "        if given_training:\n",
        "            self.model.train()\n",
        "\n",
        "    def save(self, data_loader, save_path=None, verbose=True, return_verbose=False,\n",
        "             save_predictions=False, save_clean_inputs=False, save_type='float'):\n",
        "        r\"\"\"\n",
        "        Save adversarial inputs as torch.tensor from given torch.utils.data.DataLoader.\n",
        "        Arguments:\n",
        "            save_path (str): save_path.\n",
        "            data_loader (torch.utils.data.DataLoader): data loader.\n",
        "            verbose (bool): True for displaying detailed information. (Default: True)\n",
        "            return_verbose (bool): True for returning detailed information. (Default: False)\n",
        "            save_predictions (bool): True for saving predicted labels (Default: False)\n",
        "            save_clean_inputs (bool): True for saving clean inputs (Default: False)\n",
        "        \"\"\"\n",
        "        if save_path is not None:\n",
        "            adv_input_list = []\n",
        "            label_list = []\n",
        "            if save_predictions:\n",
        "                pred_list = []\n",
        "            if save_clean_inputs:\n",
        "                input_list = []\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        l2_distance = []\n",
        "\n",
        "        total_batch = len(data_loader)\n",
        "        given_training = self.model.training\n",
        "\n",
        "        for step, (inputs, labels) in enumerate(data_loader):\n",
        "            start = time.time()\n",
        "            adv_inputs = self.__call__(inputs, labels)\n",
        "            batch_size = len(inputs)\n",
        "\n",
        "            if verbose or return_verbose:\n",
        "                with torch.no_grad():\n",
        "                    adv_inputs_type_changed = self.to_type(adv_inputs, 'float')\n",
        "                    outputs = self.get_output_with_eval_nograd(adv_inputs_type_changed)\n",
        "\n",
        "                    # Calculate robust accuracy\n",
        "                    _, pred = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    right_idx = (pred == labels.to(self.device))\n",
        "                    correct += right_idx.sum()\n",
        "                    rob_acc = 100 * float(correct) / total\n",
        "\n",
        "                    # Calculate l2 distance\n",
        "                    delta = (adv_inputs_type_changed - inputs.to(self.device)).view(batch_size, -1)\n",
        "                    l2_distance.append(torch.norm(delta[~right_idx], p=2, dim=1))\n",
        "                    l2 = torch.cat(l2_distance).mean().item()\n",
        "\n",
        "                    # Calculate time computation\n",
        "                    progress = (step+1)/total_batch*100\n",
        "                    end = time.time()\n",
        "                    elapsed_time = end-start\n",
        "\n",
        "                    if verbose:\n",
        "                        self._save_print(progress, rob_acc, l2, elapsed_time, end='\\r')\n",
        "\n",
        "            if save_path is not None:\n",
        "                adv_input_list.append(self.to_type(adv_inputs.detach().cpu(), save_type))\n",
        "                label_list.append(labels.detach().cpu())\n",
        "\n",
        "                adv_input_list_cat = torch.cat(adv_input_list, 0)\n",
        "                label_list_cat = torch.cat(label_list, 0)\n",
        "                save_dict = {'adv_inputs':adv_input_list_cat, 'labels':label_list_cat}\n",
        "\n",
        "                if save_predictions:\n",
        "                    pred_list.append(pred.detach().cpu())\n",
        "                    pred_list_cat = torch.cat(pred_list, 0)\n",
        "                    save_dict['preds'] = pred_list_cat\n",
        "\n",
        "                if save_clean_inputs:\n",
        "                    input_list.append(self.to_type(inputs.detach().cpu(), save_type))\n",
        "                    input_list_cat = torch.cat(input_list, 0)\n",
        "                    save_dict['clean_inputs'] = input_list_cat\n",
        "\n",
        "                save_dict['save_type'] = save_type\n",
        "                torch.save(save_dict, save_path)\n",
        "\n",
        "        # To avoid erasing the printed information.\n",
        "        if verbose:\n",
        "            self._save_print(progress, rob_acc, l2, elapsed_time, end='\\n')\n",
        "\n",
        "        if given_training:\n",
        "            self.model.train()\n",
        "\n",
        "        if return_verbose:\n",
        "            return rob_acc, l2, elapsed_time\n",
        "\n",
        "    @staticmethod\n",
        "    def _save_print(progress, rob_acc, l2, elapsed_time, end):\n",
        "        print('- Save progress: %2.2f %% / Robust accuracy: %2.2f %% / L2: %1.5f (%2.3f it/s) \\t' \\\n",
        "              % (progress, rob_acc, l2, elapsed_time), end=end)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(load_path, batch_size=128, shuffle=False,\n",
        "             load_predictions=False, load_clean_inputs=False):\n",
        "        save_dict = torch.load(load_path)\n",
        "        keys = ['adv_inputs', 'labels']\n",
        "\n",
        "        if load_predictions:\n",
        "            keys.append('preds')\n",
        "        if load_clean_inputs:\n",
        "            keys.append('clean_inputs')\n",
        "\n",
        "        if save_dict['save_type'] == 'int':\n",
        "            save_dict['adv_inputs'] = save_dict['adv_inputs'].float()/255\n",
        "            if load_clean_inputs:\n",
        "                save_dict['clean_inputs'] = save_dict['clean_inputs'].float()/255\n",
        "\n",
        "        adv_data = TensorDataset(*[save_dict[key] for key in keys])\n",
        "        adv_loader = DataLoader(adv_data, batch_size=batch_size, shuffle=shuffle)\n",
        "        print(\"Data is loaded in the following order: [%s]\"%(\", \".join(keys)))\n",
        "        return adv_loader\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_output_with_eval_nograd(self, inputs):\n",
        "        given_training = self.model.training\n",
        "        if given_training:\n",
        "            self.model.eval()\n",
        "        outputs = self.get_logits(inputs)\n",
        "        if given_training:\n",
        "            self.model.train()\n",
        "        return outputs\n",
        "\n",
        "    def get_target_label(self, inputs, labels=None):\n",
        "        r\"\"\"\n",
        "        Function for changing the attack mode.\n",
        "        Return input labels.\n",
        "        \"\"\"\n",
        "        if self._target_map_function is None:\n",
        "            raise ValueError('target_map_function is not initialized by set_mode_targeted.')\n",
        "        target_labels = self._target_map_function(inputs, labels)\n",
        "        return target_labels\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_least_likely_label(self, inputs, labels=None):\n",
        "        outputs = self.get_output_with_eval_nograd(inputs)\n",
        "        if labels is None:\n",
        "            _, labels = torch.max(outputs, dim=1)\n",
        "        n_classses = outputs.shape[-1]\n",
        "\n",
        "        target_labels = torch.zeros_like(labels)\n",
        "        for counter in range(labels.shape[0]):\n",
        "            l = list(range(n_classses))\n",
        "            l.remove(labels[counter])\n",
        "            _, t = torch.kthvalue(outputs[counter][l], self._kth_min)\n",
        "            target_labels[counter] = l[t]\n",
        "\n",
        "        return target_labels.long().to(self.device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_random_target_label(self, inputs, labels=None):\n",
        "        outputs = self.get_output_with_eval_nograd(inputs)\n",
        "        if labels is None:\n",
        "            _, labels = torch.max(outputs, dim=1)\n",
        "        n_classses = outputs.shape[-1]\n",
        "\n",
        "        target_labels = torch.zeros_like(labels)\n",
        "        for counter in range(labels.shape[0]):\n",
        "            l = list(range(n_classses))\n",
        "            l.remove(labels[counter])\n",
        "            t = (len(l)*torch.rand([1])).long().to(self.device)\n",
        "            target_labels[counter] = l[t]\n",
        "\n",
        "        return target_labels.long().to(self.device)\n",
        "\n",
        "    @staticmethod\n",
        "    def to_type(inputs, type):\n",
        "        r\"\"\"\n",
        "        Return inputs as int if float is given.\n",
        "        \"\"\"\n",
        "        if type == 'int':\n",
        "            if isinstance(inputs, torch.FloatTensor) or isinstance(inputs, torch.cuda.FloatTensor):\n",
        "                return (inputs*255).type(torch.uint8)\n",
        "        elif type == 'float':\n",
        "            if isinstance(inputs, torch.ByteTensor) or isinstance(inputs, torch.cuda.ByteTensor):\n",
        "                return inputs.float()/255\n",
        "        else:\n",
        "            raise ValueError(type + \" is not a valid type. [Options: float, int]\")\n",
        "        return inputs\n",
        "\n",
        "    def __call__(self, inputs, labels=None, *args, **kwargs):\n",
        "        given_training = self.model.training\n",
        "        self._change_model_mode(given_training)\n",
        "\n",
        "        if self._normalization_applied is True:\n",
        "            inputs = self.inverse_normalize(inputs)\n",
        "            self._set_normalization_applied(False)\n",
        "\n",
        "            adv_inputs = self.forward(inputs, labels, *args, **kwargs)\n",
        "            adv_inputs = self.to_type(adv_inputs, self.return_type)\n",
        "\n",
        "            adv_inputs = self.normalize(adv_inputs)\n",
        "            self._set_normalization_applied(True)\n",
        "        else:\n",
        "            adv_inputs = self.forward(inputs, labels, *args, **kwargs)\n",
        "            adv_inputs = self.to_type(adv_inputs, self.return_type)\n",
        "\n",
        "        self._recover_model_mode(given_training)\n",
        "\n",
        "        return adv_inputs\n",
        "\n",
        "    def __repr__(self):\n",
        "        info = self.__dict__.copy()\n",
        "\n",
        "        del_keys = ['model', 'attack', 'supported_mode']\n",
        "\n",
        "        for key in info.keys():\n",
        "            if key[0] == \"_\":\n",
        "                del_keys.append(key)\n",
        "\n",
        "        for key in del_keys:\n",
        "            del info[key]\n",
        "\n",
        "        info['attack_mode'] = self.attack_mode\n",
        "        info['return_type'] = self.return_type\n",
        "        info['normalization_used'] = True if self.normalization_used is not None else False\n",
        "\n",
        "        return self.attack + \"(\" + ', '.join('{}={}'.format(key, val) for key, val in info.items()) + \")\"\n",
        "\n",
        "    def __setattr__(self, name, value):\n",
        "        object.__setattr__(self, name, value)\n",
        "        \n",
        "        attacks = self.__dict__.get('_attacks')\n",
        "\n",
        "        # Get all items in iterable items.\n",
        "        def get_all_values(items, stack=[]):\n",
        "            if (items not in stack):\n",
        "                stack.append(items)\n",
        "                if isinstance(items, list) or isinstance(items, dict):\n",
        "                    if isinstance(items, dict):\n",
        "                        items = (list(items.keys())+list(items.values()))\n",
        "                    for item in items:\n",
        "                        yield from get_all_values(item, stack)\n",
        "                else:\n",
        "                    if isinstance(items, Attack):\n",
        "                        yield items\n",
        "            else:\n",
        "                if isinstance(items, Attack):\n",
        "                    yield items\n",
        "                \n",
        "        for num, value in enumerate(get_all_values(value)):\n",
        "            attacks[name+\".\"+str(num)] = value\n",
        "            for subname, subvalue in value.__dict__.get('_attacks').items():\n",
        "                attacks[name+\".\"+subname] = subvalue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class CW(Attack):\n",
        "    r\"\"\"\n",
        "    CW in the paper 'Towards Evaluating the Robustness of Neural Networks'\n",
        "    [https://arxiv.org/abs/1608.04644]\n",
        "\n",
        "    Distance Measure : L2\n",
        "\n",
        "    Arguments:\n",
        "        model (nn.Module): model to attack.\n",
        "        c (float): c in the paper. parameter for box-constraint. (Default: 1)    \n",
        "            :math:`minimize \\Vert\\frac{1}{2}(tanh(w)+1)-x\\Vert^2_2+c\\cdot f(\\frac{1}{2}(tanh(w)+1))`\n",
        "        kappa (float): kappa (also written as 'confidence') in the paper. (Default: 0)\n",
        "            :math:`f(x')=max(max\\{Z(x')_i:i\\neq t\\} -Z(x')_t, - \\kappa)`\n",
        "        steps (int): number of steps. (Default: 50)\n",
        "        lr (float): learning rate of the Adam optimizer. (Default: 0.01)\n",
        "\n",
        "    .. warning:: With default c, you can't easily get adversarial images. Set higher c like 1.\n",
        "\n",
        "    Shape:\n",
        "        - images: :math:`(N, C, H, W)` where `N = number of batches`, `C = number of channels`,        `H = height` and `W = width`. It must have a range [0, 1].\n",
        "        - labels: :math:`(N)` where each value :math:`y_i` is :math:`0 \\leq y_i \\leq` `number of labels`.\n",
        "        - output: :math:`(N, C, H, W)`.\n",
        "\n",
        "    Examples::\n",
        "        >>> attack = torchattacks.CW(model, c=1, kappa=0, steps=50, lr=0.01)\n",
        "        >>> adv_images = attack(images, labels)\n",
        "\n",
        "    .. note:: Binary search for c is NOT IMPLEMENTED methods in the paper due to time consuming.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, model, c=1, kappa=0, steps=50, lr=0.01):\n",
        "        super().__init__(\"CW\", model)\n",
        "        self.c = c\n",
        "        self.kappa = kappa\n",
        "        self.steps = steps\n",
        "        self.lr = lr\n",
        "        self.supported_mode = ['default', 'targeted']\n",
        "\n",
        "    def forward(self, images, labels):\n",
        "        r\"\"\"\n",
        "        Overridden.\n",
        "        \"\"\"\n",
        "        images = images.clone().detach().to(self.device)\n",
        "        labels = labels.clone().detach().to(self.device)\n",
        "\n",
        "        if self.targeted:\n",
        "            target_labels = self.get_target_label(images, labels)\n",
        "\n",
        "        # w = torch.zeros_like(images).detach() # Requires 2x times\n",
        "        w = self.inverse_tanh_space(images).detach()\n",
        "        w.requires_grad = True\n",
        "\n",
        "        best_adv_images = images.clone().detach()\n",
        "        best_L2 = 1e10*torch.ones((len(images))).to(self.device)\n",
        "        prev_cost = 1e10\n",
        "        dim = len(images.shape)\n",
        "\n",
        "        MSELoss = nn.MSELoss(reduction='none')\n",
        "        Flatten = nn.Flatten()\n",
        "\n",
        "        optimizer = optim.Adam([w], lr=self.lr)\n",
        "\n",
        "        for step in range(self.steps):\n",
        "            # Get adversarial images\n",
        "            adv_images = self.tanh_space(w)\n",
        "\n",
        "            # Calculate loss\n",
        "            current_L2 = MSELoss(Flatten(adv_images),\n",
        "                                 Flatten(images)).sum(dim=1)\n",
        "            L2_loss = current_L2.sum()\n",
        "\n",
        "            outputs = self.get_logits(adv_images)\n",
        "            if self.targeted:\n",
        "                f_loss = self.f(outputs, target_labels).sum()\n",
        "            else:\n",
        "                f_loss = self.f(outputs, labels).sum()\n",
        "\n",
        "            cost = L2_loss + self.c*f_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            cost.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update adversarial images\n",
        "            _, pre = torch.max(outputs.detach(), 1)\n",
        "            correct = (pre == labels).float()\n",
        "\n",
        "            # filter out images that get either correct predictions or non-decreasing loss, \n",
        "            # i.e., only images that are both misclassified and loss-decreasing are left \n",
        "            mask = (1-correct)*(best_L2 > current_L2.detach())\n",
        "            best_L2 = mask*current_L2.detach() + (1-mask)*best_L2\n",
        "\n",
        "            mask = mask.view([-1]+[1]*(dim-1))\n",
        "            best_adv_images = mask*adv_images.detach() + (1-mask)*best_adv_images\n",
        "\n",
        "            # Early stop when loss does not converge.\n",
        "            # max(.,1) To prevent MODULO BY ZERO error in the next step.\n",
        "            if step % max(self.steps//10,1) == 0:\n",
        "                if cost.item() > prev_cost:\n",
        "                    return best_adv_images\n",
        "                prev_cost = cost.item()\n",
        "\n",
        "        return best_adv_images\n",
        "\n",
        "    def tanh_space(self, x):\n",
        "        return 1/2*(torch.tanh(x) + 1)\n",
        "\n",
        "    def inverse_tanh_space(self, x):\n",
        "        # torch.atanh is only for torch >= 1.7.0\n",
        "        return self.atanh(x*2-1)\n",
        "\n",
        "    def atanh(self, x):\n",
        "        return 0.5*torch.log((1+x)/(1-x))\n",
        "\n",
        "    # f-function in the paper\n",
        "    def f(self, outputs, labels):\n",
        "        one_hot_labels = torch.eye(len(outputs[0]))[labels].to(self.device)\n",
        "\n",
        "        i, _ = torch.max((1-one_hot_labels)*outputs, dim=1) # get the second largest logit\n",
        "        j = torch.masked_select(outputs, one_hot_labels.bool()) # get the largest logit\n",
        "\n",
        "        if self.targeted:\n",
        "            return torch.clamp((i-j), min=-self.kappa)\n",
        "        else:\n",
        "            return torch.clamp((j-i), min=-self.kappa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for +: 'type' and 'str'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/IPython/core/formatters.py:706\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    699\u001b[0m stream \u001b[39m=\u001b[39m StringIO()\n\u001b[1;32m    700\u001b[0m printer \u001b[39m=\u001b[39m pretty\u001b[39m.\u001b[39mRepresentationPrinter(stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    701\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnewline,\n\u001b[1;32m    702\u001b[0m     max_seq_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_seq_length,\n\u001b[1;32m    703\u001b[0m     singleton_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msingleton_printers,\n\u001b[1;32m    704\u001b[0m     type_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype_printers,\n\u001b[1;32m    705\u001b[0m     deferred_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 706\u001b[0m printer\u001b[39m.\u001b[39;49mpretty(obj)\n\u001b[1;32m    707\u001b[0m printer\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    708\u001b[0m \u001b[39mreturn\u001b[39;00m stream\u001b[39m.\u001b[39mgetvalue()\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[39mreturn\u001b[39;00m meth(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[39mand\u001b[39;00m callable(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[39mreturn\u001b[39;00m _repr_pprint(obj, \u001b[39mself\u001b[39;49m, cycle)\n\u001b[1;32m    412\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_pprint(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
            "File \u001b[0;32m/volumes1/thesis/notebooks/notebook_env/lib/python3.8/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[39m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[39m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mrepr\u001b[39;49m(obj)\n\u001b[1;32m    779\u001b[0m lines \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[39mwith\u001b[39;00m p\u001b[39m.\u001b[39mgroup():\n",
            "Cell \u001b[0;32mIn [49], line 444\u001b[0m, in \u001b[0;36mAttack.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m info[\u001b[39m'\u001b[39m\u001b[39mreturn_type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_type\n\u001b[1;32m    442\u001b[0m info[\u001b[39m'\u001b[39m\u001b[39mnormalization_used\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalization_used \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattack \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m(\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(key, val) \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m info\u001b[39m.\u001b[39mitems()) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'type' and 'str'"
          ]
        }
      ],
      "source": [
        "Attack(name = CW, model= model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/defences/trainer.html#adversarial-training-certified-pytorch"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 ('notebook_env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1e104bd40b3717451915a48eb0e1578acd631c83a179c8bd7a28647fdfca76f1"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "007c5c3eac20492a8ea516282a38af93": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b5f5d84ca7b4892a116d3c1809c3900": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d0304a706b34fc782b1615702a1313f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e2a0c33c3af46ccab2a8c1bb52cdb47": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ed1635161f9465d929321bdf5a61bb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1103c60a8f1547eaa1b984680e99aca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1da7641faf794db6aceb148d770ee5b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2843059f898c40c5905d6cc773c44186": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d6b9502aa2b44248a3b7b2cb960b416": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31b5ff05bf3444aebb5e487bf14a206a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44120a9964c440f9a10e84b8713d7f30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "466bfd965bdb4faabce67e0a514c4be7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47a4f2100ea74ff18b3995e0a2ac9b88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d4e1deafdd94b62a0ea6684d314afed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3d2b36ffc3047a285c032b080a7238c",
              "IPY_MODEL_e57480d4db9c40ab8265545d321f4e78",
              "IPY_MODEL_6e13f1b052904eb5a238ac7f699ed613"
            ],
            "layout": "IPY_MODEL_007c5c3eac20492a8ea516282a38af93"
          }
        },
        "556cf98981ec4fff8e43109a5df15108": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "611704eecac2468090acdf34f968398a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "667aff09a27340d99aa6a349e126f467": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b5f5d84ca7b4892a116d3c1809c3900",
            "max": 187490228,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ed1635161f9465d929321bdf5a61bb5",
            "value": 187490228
          }
        },
        "6800badfc70d4a88a9fafe792e90e865": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e13f1b052904eb5a238ac7f699ed613": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e2a0c33c3af46ccab2a8c1bb52cdb47",
            "placeholder": "​",
            "style": "IPY_MODEL_556cf98981ec4fff8e43109a5df15108",
            "value": " 99620/99620 [00:00&lt;00:00, 277844.68it/s]"
          }
        },
        "7d2c234dfd024d29adbdfa8bd0a8e2c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8070320f4200456b87eb791abdbcaf63": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e382e0c517440f8b2523a3be3bbbb5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2d743c8449b40cf8cfb3e035e61e955",
              "IPY_MODEL_f017d0bda5884853af951976473fbdfd",
              "IPY_MODEL_fbd35017269f42f09dfd3d599e3c6f5e"
            ],
            "layout": "IPY_MODEL_47a4f2100ea74ff18b3995e0a2ac9b88"
          }
        },
        "920ac0b645d14201b6f5b4c5eaada5bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1da7641faf794db6aceb148d770ee5b3",
            "placeholder": "​",
            "style": "IPY_MODEL_6800badfc70d4a88a9fafe792e90e865",
            "value": "100%"
          }
        },
        "a29e4250cf2b4740b22d45c4b49dbde4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_920ac0b645d14201b6f5b4c5eaada5bf",
              "IPY_MODEL_667aff09a27340d99aa6a349e126f467",
              "IPY_MODEL_e6b44dd988d44a2986148b05697cd117"
            ],
            "layout": "IPY_MODEL_611704eecac2468090acdf34f968398a"
          }
        },
        "a2d743c8449b40cf8cfb3e035e61e955": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44120a9964c440f9a10e84b8713d7f30",
            "placeholder": "​",
            "style": "IPY_MODEL_7d2c234dfd024d29adbdfa8bd0a8e2c8",
            "value": "100%"
          }
        },
        "d2156e5c21db40fa87db3e402c0706c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8148ba24cd6449b917480f14a76180f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e23488f6634946eea008ab05c227a664": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3d2b36ffc3047a285c032b080a7238c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8070320f4200456b87eb791abdbcaf63",
            "placeholder": "​",
            "style": "IPY_MODEL_2843059f898c40c5905d6cc773c44186",
            "value": "100%"
          }
        },
        "e57480d4db9c40ab8265545d321f4e78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8148ba24cd6449b917480f14a76180f",
            "max": 99620,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31b5ff05bf3444aebb5e487bf14a206a",
            "value": 99620
          }
        },
        "e6b44dd988d44a2986148b05697cd117": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_466bfd965bdb4faabce67e0a514c4be7",
            "placeholder": "​",
            "style": "IPY_MODEL_e23488f6634946eea008ab05c227a664",
            "value": " 187490228/187490228 [00:08&lt;00:00, 25863863.62it/s]"
          }
        },
        "f017d0bda5884853af951976473fbdfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d0304a706b34fc782b1615702a1313f",
            "max": 88978620,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d6b9502aa2b44248a3b7b2cb960b416",
            "value": 88978620
          }
        },
        "fbd35017269f42f09dfd3d599e3c6f5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2156e5c21db40fa87db3e402c0706c0",
            "placeholder": "​",
            "style": "IPY_MODEL_1103c60a8f1547eaa1b984680e99aca0",
            "value": " 88978620/88978620 [00:04&lt;00:00, 25681841.34it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
